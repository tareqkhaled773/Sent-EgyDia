{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e31LxHWzWFvD",
        "outputId": "d36bd22d-efb8-4cf2-d76a-ec95d99bd99d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True) # Mount google drive to load training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yscm708fGi3m"
      },
      "outputs": [],
      "source": [
        "# This where all the Packages are cached instead or reinstalling them every new runtime\n",
        "PACKAGES_DIR = '/content/drive/My Drive/Master1/pip_cache'\n",
        "\n",
        "# This is where MERGED & PREPROCESSED & CLEANED Dataset is\n",
        "PREPROCESSED_MERGED_DATASET_DIR = '/content/drive/MyDrive/Master1/Sentiment/PREPROCESSED_DATASET/'\n",
        "\n",
        "\n",
        "# This is where the figures are saved\n",
        "FIGURES_DIR = '/content/drive/MyDrive/Master1/Sentiment/FIGURES/FIGURES_2/'\n",
        "\n",
        "\n",
        "TEMPLATES_DIR = '/content/drive/MyDrive/Master1/Sentiment/templates/'\n",
        "\n",
        "\n",
        "\n",
        "  #########################################################################  PYTORCH ##########################################################\n",
        "\n",
        "# Directory where the pytorch models will be saved\n",
        "PY_CUSTOM_CAMEL_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/camel/py/model/model_2/model_3/'\n",
        "\n",
        "# Directory where the pytorch models will be saved\n",
        "PY_CUSTOM_CAMEL_MIX_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/camel_mix/py/model/model_2/model_3/'\n",
        "\n",
        "# Directory where the pytorch models will be saved\n",
        "PY_CUSTOM_ALANZI_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/alanzi/py/model/'\n",
        "\n",
        "# i change dropout rate from\n",
        "PY_CUSTOM_ALANZI2_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/alanzi2/py/model/'\n",
        "\n",
        "# i change dropout rate from\n",
        "PY_CUSTOM_ALANZI3_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/alanzi3/py/model/model_2/model_3/'\n",
        "\n",
        "\n",
        "# Directory where the pytorch models will be saved\n",
        "PY_CUSTOM_ARABERT_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/arabert/py/model/'\n",
        "\n",
        "\n",
        "# Directory where the pytorch models will be saved\n",
        "PY_CUSTOM_MARBERT_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/marbert/py/model/model_2/model_3/'\n",
        "\n",
        "# Directory where the purely pytorch models will be saved\n",
        "PY_ARABERT_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/models/arabert/py/model/'+\"checkpoint-19302\"\n",
        "\n",
        "# Directory where the pytorch models will be saved\n",
        "\n",
        "PY_ENSEMBLE_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/ensemble/'\n",
        "PY_ENSEMBLE_MODEL_DIR1 = '/content/drive/MyDrive/Master1/Sentiment/custom_models/ensemble1/ensemble2/ensemble3/'\n",
        "\n",
        "# Number of Figures\n",
        "FIGURE_COUNTS = 1\n",
        "\n",
        "PY_CUSTOM_Qarib_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/qarib/py/model/'\n",
        "PY_CUSTOM_Arab_bert2_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/arabbert2/py/model/'\n",
        "PY_CUSTOM_Giga_bert_MODEL_DIR = '/content/drive/MyDrive/Master1/Sentiment/custom_models/gigabert/py/model/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMXPLeRuMpd3"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVHfaOevIkCL"
      },
      "outputs": [],
      "source": [
        "!pip config set global.cache-dir /content/drive/My\\ Drive/Master1/pip_cache\n",
        "\n",
        "\n",
        "\n",
        "!pip install --cache-dir '/content/drive/My Drive/Master1/pip_cache'  transformers datasets\n",
        "!pip install --cache-dir '/content/drive/My Drive/Master1/pip_cache'  torchinfo\n",
        "!pip install --cache-dir '/content/drive/My Drive/Master1/pip_cache'  evaluate\n",
        "!pip install --cache-dir '/content/drive/My Drive/Master1/pip_cache'  pydot graphviz\n",
        "!pip install --cache-dir '/content/drive/My Drive/Master1/pip_cache'  accelerate\n",
        "\n",
        "\n",
        "\n",
        "# !pip install --cache-dir '/content/drive/My Drive/Bachelor/pip_cache' wordcloud\n",
        "# !pip install --cache-dir '/content/drive/My Drive/Bachelor/pip_cache' ar_wordcloud\n",
        "# !pip install --cache-dir '/content/drive/My Drive/Bachelor/pip_cache' python-bidi\n",
        "# !pip install --cache-dir '/content/drive/My Drive/Bachelor/pip_cache' arabic_reshaper\n",
        "# !pip install --cache-dir '/content/drive/My Drive/Bachelor/pip_cache' arabic-reshaper\n",
        "# !pip install --cache-dir '/content/drive/My Drive/Bachelor/pip_cache' autokeras\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output() # clear output window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKcoTmNZISdm"
      },
      "outputs": [],
      "source": [
        "# Packages\n",
        "# types in python\n",
        "from typing import List, Tuple ,Dict ,Any , Union\n",
        "import time\n",
        "import string\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Data Manipulating  & Preprocessing packages\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import unicodedata # normlization of arabic letters encoding to be unicoded\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None) # Setting the display option to show the full width of columns in pandas dataframe.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tensorflow as tf\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import roc_auc_score,f1_score,confusion_matrix,roc_curve\n",
        "import evaluate\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# visualization library\n",
        "pd.plotting.register_matplotlib_converters()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 14, 6\n",
        "\n",
        "# Random Seed\n",
        "RANDOM_SEED=42\n",
        "RANDOM_STATE=42\n",
        "\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "  RANDOM_SEED=seed\n",
        "  RANDOM_STATE=seed\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic=True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "\n",
        "# Set Matplotlib defaults\n",
        "plt.style.use('ggplot')\n",
        "#plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\n",
        "    \"axes\",\n",
        "    labelweight=\"bold\",\n",
        "    labelsize=\"large\",\n",
        "    titleweight=\"bold\",\n",
        "    titlesize=14,\n",
        "    titlepad=10,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxcJHYkFpNsE"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "> we will define general constants for Pytorch(Dynamic Computation Graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9JLrEECf60N"
      },
      "source": [
        "#### Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9IU-PL1WbSK"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "\n",
        "# batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# number of epochs\n",
        "EPOCHS = 10\n",
        "\n",
        "# max length of sequences\n",
        "MAX_LEN = 80\n",
        "\n",
        "# Learning Rate of the optimizers\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "\n",
        "# Default Epsilon\n",
        "EPSILON =1e-8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqyzSmBXp-Wb"
      },
      "source": [
        "#### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-m6zkzdpPp2"
      },
      "outputs": [],
      "source": [
        "#from datasets import load_metric\n",
        "import evaluate\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score,accuracy_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds:Tuple[np.ndarray, np.ndarray]) -> Dict[str, Any]:\n",
        "  \"\"\"  called each time the model is evaluated on the validation dataset\n",
        "        @returns a dictionary of metrics such as accuracy(glu) , acc_score , F1 score, precision,recall,roc_score and auc_score.\"\"\"\n",
        "  logits, labels = eval_preds\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  # f1-score\n",
        "  f1_macro = f1_score(labels, predictions, average='macro')\n",
        "  f1_weighted = f1_score(labels, predictions, average='weighted')\n",
        "  # precision\n",
        "  precision_macro = precision_score(labels, predictions, average='macro')\n",
        "  precision_weighted = precision_score(labels, predictions, average='weighted')\n",
        "  # recall\n",
        "  recall_macro = recall_score(labels, predictions, average='macro')\n",
        "  recall_weighted = recall_score(labels, predictions, average='weighted')\n",
        "\n",
        "  #acc_score=accuracy_score(labels,predictions)\n",
        "\n",
        "  metric = evaluate.load(\"glue\", \"sst2\")\n",
        "  metrics = metric.compute(predictions=predictions, references=labels)\n",
        "  metrics.update({\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"f1_weighted\": f1_weighted,\n",
        "        \"precision_macro\": precision_macro,\n",
        "        \"precision_weighted\": precision_weighted,\n",
        "        \"recall_macro\": recall_macro,\n",
        "        \"recall_weighted\": recall_weighted,\n",
        "      #  'accuracy_score':acc_score\n",
        "        })\n",
        "  return metrics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik2UPFVafvV4"
      },
      "source": [
        "## Transformers using Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmHXoCZSITw_"
      },
      "source": [
        "### Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j09cclMHIQHU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import AutoConfig,AutoTokenizer,TFAutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback,AutoModelForSequenceClassification\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikHQBnVdfub_"
      },
      "source": [
        "### Clear Previous Sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8fdwDbe0PDe"
      },
      "outputs": [],
      "source": [
        "from keras.backend import clear_session # clears the current Keras session and frees up memory.\n",
        "import gc\n",
        "clear_session()\n",
        "\n",
        "# set the maximum split size to 512 MB\n",
        "max_split_size_mb = 512\n",
        "\n",
        "\n",
        "# Clear PyTorch cache (if using a GPU)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Delete existing PyTorch models or variables\n",
        "\n",
        "model = 'dummy'\n",
        "optimizer= 'dummy'\n",
        "tokenizer='dummy'\n",
        "trainer='dummy'\n",
        "del model, optimizer,tokenizer,trainer\n",
        "\n",
        "# set the environment variable\n",
        "#os.environ['PYTORCH_CUDA_ALLOC_CONF'] = f'max_split_size={max_split_size_mb}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BJvfmhu3K-9",
        "outputId": "0521b277-076f-4f5d-a280-5eef19f4c98e"
      },
      "outputs": [],
      "source": [
        "#  check if your system has a compatible NVIDIA G\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbEfkWjQoot7"
      },
      "source": [
        "### My Preprocessed Labeled Dataset\n",
        "> Consists of Egyptian and Modern Standard Arabic (MSA) .\n",
        "\n",
        ">\n",
        "```\n",
        "{'LABEL': Value(dtype='int64', id=None),\n",
        " 'TWEET': Value(dtype='string', id=None)}\n",
        "```\n",
        "> Label explaination :\n",
        "\n",
        "```\n",
        "SENTIMENT_TO_ID = {\n",
        "   \"positive\": 0 ,\n",
        "   \"negative\": 1,\n",
        "   \"neutral\": 2,\n",
        "}\n",
        "\n",
        "ID_TO_SENTIMENT = {\n",
        "   0:\"positive\" ,\n",
        "   1:\"negative\" ,\n",
        "   2:\"neutral\",\n",
        "   }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsidSelFoot9"
      },
      "outputs": [],
      "source": [
        "SENTIMENT_TO_ID = {\n",
        "   \"positive\": 0 ,\n",
        "   \"negative\": 1,\n",
        "   \"neutral\": 2,\n",
        "}\n",
        "\n",
        "ID_TO_SENTIMENT = {\n",
        "   0:\"positive\" ,\n",
        "   1:\"negative\" ,\n",
        "   2:\"neutral\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcnguBCuoot-"
      },
      "outputs": [],
      "source": [
        "# # Dataset names\n",
        "# TRAIN_DATASET_NAME='TRAIN_DATASET'\n",
        "# VALIDDATION_DATASET_NAME='VALIDATION_DATASET'\n",
        "# TEST_DATASET_NAME='TEST_DATASET'\n",
        "# DATASET_NAME = 'DATASET'\n",
        "\n",
        "# stemmed_data_files = {\"train\": f\"{os.path.join(PREPROCESSED_MERGED_DATASET_DIR , TRAIN_DATASET_NAME + '.csv')}\",\n",
        "#               'validation':f\"{os.path.join(PREPROCESSED_MERGED_DATASET_DIR, VALIDDATION_DATASET_NAME + '.csv' )}\",\n",
        "#               \"test\": f\"{os.path.join(PREPROCESSED_MERGED_DATASET_DIR ,TEST_DATASET_NAME + '.csv')}\",\n",
        "#               #\"dataset\": f\"{PREPROCESSED_MERGED_DATASET_DIR +DATASET_NAME}.csv\",\n",
        "#               }\n",
        "\n",
        "# unstemmed_data_files = {\"train\": f\"{os.path.join(PREPROCESSED_MERGED_DATASET_DIR , 'unstemmed/'+TRAIN_DATASET_NAME + '.csv')}\",\n",
        "#               'validation':f\"{os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/'+VALIDDATION_DATASET_NAME + '.csv' )}\",\n",
        "#               'test':f\"{os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/'+ TEST_DATASET_NAME + '.csv' )}\",\n",
        "#               #\"dataset\": f\"{PREPROCESSED_MERGED_DATASET_DIR +DATASET_NAME}.csv\",\n",
        "#               }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnCgtBvy8KLr"
      },
      "outputs": [],
      "source": [
        "# from datasets import Dataset, DatasetDict\n",
        "# import pandas as pd\n",
        "# import os\n",
        "\n",
        "# # Dataset names\n",
        "# TRAIN_DATASET_NAME = 'TRAIN_DATASET'\n",
        "# VALIDDATION_DATASET_NAME = 'VALIDATION_DATASET'\n",
        "# TEST_DATASET_NAME = 'TEST_DATASET'\n",
        "# DATASET_NAME = 'DATASET'\n",
        "\n",
        "# # Data file paths\n",
        "# stemmed_data_files = {\n",
        "#     \"train\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, TRAIN_DATASET_NAME + '.csv'),\n",
        "#     \"validation\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, VALIDDATION_DATASET_NAME + '.csv'),\n",
        "#     \"test\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, TEST_DATASET_NAME + '.csv'),\n",
        "# }\n",
        "\n",
        "# unstemmed_data_files = {\n",
        "#     \"train\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/' + TRAIN_DATASET_NAME + '.csv'),\n",
        "#     \"validation\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/' + VALIDDATION_DATASET_NAME + '.csv'),\n",
        "#     \"test\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/' + TEST_DATASET_NAME + '.csv'),\n",
        "# }\n",
        "\n",
        "# # Function to convert CSVs to Hugging Face DatasetDict (with 10% sampling)\n",
        "# def load_csv_as_datasetdict(data_files: dict, sample_ratio: float = 0.5) -> DatasetDict:\n",
        "#     # Load each split using pandas\n",
        "#     train_df = pd.read_csv(data_files[\"train\"])\n",
        "#     val_df = pd.read_csv(data_files[\"validation\"])\n",
        "#     test_df = pd.read_csv(data_files[\"test\"])\n",
        "\n",
        "#     # Sample 10% of each split (shuffle first for randomness)\n",
        "#     train_df = train_df.sample(frac=sample_ratio, random_state=42).reset_index(drop=True)\n",
        "#     val_df = val_df.sample(frac=sample_ratio, random_state=42).reset_index(drop=True)\n",
        "#     test_df = test_df.sample(frac=sample_ratio, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#     # Convert to Hugging Face Datasets\n",
        "#     train_dataset = Dataset.from_pandas(train_df)\n",
        "#     val_dataset = Dataset.from_pandas(val_df)\n",
        "#     test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "#     return DatasetDict({\n",
        "#         \"train\": train_dataset,\n",
        "#         \"validation\": val_dataset,\n",
        "#         \"test\": test_dataset\n",
        "#     })\n",
        "\n",
        "# # Load both datasets with 10% of the data\n",
        "# stemmed_dataset = load_csv_as_datasetdict(stemmed_data_files, sample_ratio=0.5)\n",
        "# dataset = load_csv_as_datasetdict(unstemmed_data_files, sample_ratio=0.5)\n",
        "\n",
        "# # Optional: Check sample\n",
        "# print(\" Stemmed Dataset (10%):\", stemmed_dataset)\n",
        "# print(\" Unstemmed Dataset (10%):\", dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dMBBNsHaYkn9",
        "outputId": "b95fd9f2-da62-4d46-c959-06a14681a7fe"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Dataset names\n",
        "TRAIN_DATASET_NAME = 'TRAIN_DATASET'\n",
        "VALIDDATION_DATASET_NAME = 'VALIDATION_DATASET'\n",
        "TEST_DATASET_NAME = 'TEST_DATASET'\n",
        "DATASET_NAME = 'DATASET'\n",
        "\n",
        "# Data file paths\n",
        "stemmed_data_files = {\n",
        "    \"train\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, TRAIN_DATASET_NAME + '.csv'),\n",
        "    \"validation\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, VALIDDATION_DATASET_NAME + '.csv'),\n",
        "    \"test\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, TEST_DATASET_NAME + '.csv'),\n",
        "}\n",
        "\n",
        "unstemmed_data_files = {\n",
        "    \"train\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/' + TRAIN_DATASET_NAME + '.csv'),\n",
        "    \"validation\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/' + VALIDDATION_DATASET_NAME + '.csv'),\n",
        "    \"test\": os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/' + TEST_DATASET_NAME + '.csv'),\n",
        "}\n",
        "\n",
        "# Function to convert CSVs to Hugging Face DatasetDict (with 10% sampling)\n",
        "# def load_csv_as_datasetdict(data_files: dict, sample_ratio: float = 0.5) -> DatasetDict:\n",
        "#     # Load each split using pandas\n",
        "#     train_df = pd.read_csv(data_files[\"train\"])\n",
        "#     val_df = pd.read_csv(data_files[\"validation\"])\n",
        "#     test_df = pd.read_csv(data_files[\"test\"])\n",
        "\n",
        "#     # Sample 10% of each split (shuffle first for randomness)\n",
        "#     train_df = train_df.sample(frac=sample_ratio, random_state=42).reset_index(drop=True)\n",
        "#     val_df = val_df.sample(frac=sample_ratio, random_state=42).reset_index(drop=True)\n",
        "#     test_df = test_df.sample(frac=sample_ratio, random_state=42).reset_index(drop=True)\n",
        "\n",
        "#     # Convert to Hugging Face Datasets\n",
        "#     train_dataset = Dataset.from_pandas(train_df)\n",
        "#     val_dataset = Dataset.from_pandas(val_df)\n",
        "#     test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "#     return DatasetDict({\n",
        "#         \"train\": train_dataset,\n",
        "#         \"validation\": val_dataset,\n",
        "#         \"test\": test_dataset\n",
        "#     })\n",
        "\n",
        "# # Load both datasets with 10% of the data\n",
        "# stemmed_dataset = load_csv_as_datasetdict(stemmed_data_files, sample_ratio=0.5)\n",
        "# dataset = load_csv_as_datasetdict(unstemmed_data_files, sample_ratio=0.5)\n",
        "\n",
        "# # Optional: Check sample\n",
        "# print(\" Stemmed Dataset (10%):\", stemmed_dataset)\n",
        "# print(\" Unstemmed Dataset (10%):\", dataset)\n",
        "\n",
        "\n",
        "#  Function to convert CSVs to Hugging Face DatasetDict\n",
        "def load_csv_as_datasetdict(data_files: dict) -> DatasetDict:\n",
        "    # Load each split using pandas\n",
        "    train_df = pd.read_csv(data_files[\"train\"])\n",
        "    val_df = pd.read_csv(data_files[\"validation\"])\n",
        "    test_df = pd.read_csv(data_files[\"test\"])\n",
        "\n",
        "    # Convert to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    return DatasetDict({\n",
        "        \"train\": train_dataset,\n",
        "        \"validation\": val_dataset,\n",
        "        \"test\": test_dataset\n",
        "    })\n",
        "\n",
        "#  Load both datasets\n",
        "stemmed_dataset = load_csv_as_datasetdict(stemmed_data_files)\n",
        "dataset = load_csv_as_datasetdict(unstemmed_data_files)\n",
        "\n",
        "#  Optional: Check sample\n",
        "print(\" Stemmed Dataset:\", stemmed_dataset)\n",
        "print(\" Unstemmed Dataset:\", dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8g5mNOToouB",
        "outputId": "89b7f14c-5e7d-4e5d-c2f9-1c2033fb7d8d"
      },
      "outputs": [],
      "source": [
        "# explore Features\n",
        "dataset['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FhKRP04voouC",
        "outputId": "a847bd56-2c84-4f12-9503-8ed291717db2"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(dataset['train'])\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsejNWThIsjN",
        "outputId": "62f8e175-2496-42d9-a58b-a3a12a920689"
      },
      "outputs": [],
      "source": [
        "# prompt: write the code to count samples of each class?\n",
        "\n",
        "# Function to count samples of each class in a dataset split\n",
        "def count_class_samples(dataset_split: Dataset) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Counts the number of samples for each class in a dataset split.\n",
        "\n",
        "    Args:\n",
        "        dataset_split: A Hugging Face Dataset object representing a dataset split.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are class labels (integers) and values are their counts.\n",
        "    \"\"\"\n",
        "    return dict(Counter(dataset_split['LABEL']))\n",
        "\n",
        "# Count samples for each split in the 'dataset' (unstemmed)\n",
        "train_class_counts = count_class_samples(dataset['train'])\n",
        "validation_class_counts = count_class_samples(dataset['validation'])\n",
        "test_class_counts = count_class_samples(dataset['test'])\n",
        "\n",
        "print(\"Class distribution in training set:\", train_class_counts)\n",
        "print(\"Class distribution in validation set:\", validation_class_counts)\n",
        "print(\"Class distribution in test set:\", test_class_counts)\n",
        "\n",
        "# You can also count for the stemmed dataset if needed\n",
        "# stemmed_train_class_counts = count_class_samples(stemmed_dataset['train'])\n",
        "# print(\"Class distribution in stemmed training set:\", stemmed_train_class_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "6Su3mGdaeLuo",
        "outputId": "9dbf2a6b-76ae-4b67-f8d7-0a8d0bf2d27a"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "Tweet_Tokenizer = TweetTokenizer()\n",
        "df['TWEET'].map(lambda tweet: len(Tweet_Tokenizer.tokenize(tweet))).hist(bins = 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyec5dCk1V37"
      },
      "source": [
        "#### Data Collator\n",
        "\n",
        "> Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0tMDmnWI5jH"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Tuple\n",
        "from torch.utils.data import DataLoader, Dataset,RandomSampler,SequentialSampler,TensorDataset\n",
        "\n",
        "def data_loader(dataset:Dataset,\n",
        "                tokenizer:AutoTokenizer) -> Tuple[DataLoader, DataLoader,DataLoader]:\n",
        "    # Convert labels data types to torch.Tensor\n",
        "  train_labels = torch.tensor(dataset['train']['LABEL'])\n",
        "  val_labels = torch.tensor(dataset['validation']['LABEL'])\n",
        "  test_labels = torch.tensor(dataset['test']['LABEL'])\n",
        "\n",
        "\n",
        "  train_inputs, train_masks = preprocessing_for_bert(tokenizer = tokenizer,\n",
        "                                                    text_preprocessing_fn = None,\n",
        "                                                    data = dataset['train']['TWEET'])\n",
        "  val_inputs, val_masks = preprocessing_for_bert(tokenizer = tokenizer,\n",
        "                                                text_preprocessing_fn = None,\n",
        "                                                data = dataset['validation']['TWEET'])\n",
        "  test_inputs, test_masks = preprocessing_for_bert(tokenizer = tokenizer,\n",
        "                                                text_preprocessing_fn = None,\n",
        "                                                data = dataset['test']['TWEET'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # randomly samples elements from a dataset without replacement\n",
        "  # Create the DataLoader for our training set\n",
        "  train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "  # defines the strategy to draw Random samples from the dataset\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data, sampler=train_sampler,  batch_size=BATCH_SIZE , drop_last=True\n",
        "  )\n",
        "\n",
        "  # Create the DataLoader for our validation set\n",
        "  val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "  # samples elements from a given sequence in a sequential order\n",
        "  # defines the strategy to draw Sequential samples from the dataset\n",
        "  eval_sampler = SequentialSampler(val_data)\n",
        "  eval_dataloader = DataLoader(\n",
        "      val_data, sampler=eval_sampler, batch_size=BATCH_SIZE\n",
        "  )\n",
        "  # Create the TensorDataset for our test set\n",
        "  test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "\n",
        "\n",
        "  # Create the DataLoader for our test set\n",
        "  test_sampler = SequentialSampler(test_dataset)\n",
        "\n",
        "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "  del dataset\n",
        "\n",
        "  return train_dataloader , eval_dataloader ,test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxmOpxKUdcKJ"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9ChNcnQdfKt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm.auto import tqdm as ProgressBar\n",
        "\n",
        "def preprocessing_for_bert(tokenizer: AutoTokenizer,\n",
        "                           data: np.array ,\n",
        "                           text_preprocessing_fn) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    tokenizer (AutoTokenizer): Instance of BERT tokenizer.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @param    model_name (str): Name of BERT model used for tokenization.\n",
        "    @param    text_preprocessing_fn: Function to preprocess text data.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "   \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "\n",
        "    # Progress bar for tracking progress\n",
        "    progress_bar = ProgressBar(range(len(data)))\n",
        "\n",
        "    # For every sentence...\n",
        "    for i, sentence in enumerate(data):\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text= sentence,  #text_preprocessing_fn(sentence),  # Preprocess sentence\n",
        "            add_special_tokens = True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length = MAX_LEN,             # Max length to truncate/pad\n",
        "            padding='max_length',\n",
        "            #pad_to_max_length=True,         # Pad sentence to max length\n",
        "            return_attention_mask = True,     # Return attention mask\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86RcmAILabTs"
      },
      "source": [
        "### Model Initialization - BertClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hssP5ACKdGJ"
      },
      "outputs": [],
      "source": [
        "from typing import Optional , Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel , AutoConfig\n",
        "\n",
        "import pickle\n",
        "\n",
        "def load_model_from_checkpoint(model:nn.Module,save_dir:Optional[str] = None,checkpoint:Optional[str]= None):\n",
        "      \"\"\"\n",
        "      load model from specific checkpoint\n",
        "      \"\"\"\n",
        "      if save_dir is not None and checkpoint is not None:\n",
        "        checkpoint_path = os.path.join(save_dir,checkpoint)\n",
        "        print(f'Loading model from this checkpoint {checkpoint_path}')\n",
        "\n",
        "        checkpoint = torch.load(checkpoint_path,map_location=device)\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "        print(f'loaded checkpoint of {model.model_name} successfully')\n",
        "      return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpg1FTkO8mxF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT-based classifier model for binary classification\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str,\n",
        "                 freeze_bert: bool = False,\n",
        "                 hidden_size: int = 256,\n",
        "                 num_hidden_layers: int = 2,\n",
        "                 dropout_prob: Optional[float] = None ,\n",
        "                 label2id: Optional[Dict[str,int]] = SENTIMENT_TO_ID,\n",
        "                 id2label: Optional[Dict[int,str]] = ID_TO_SENTIMENT,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        @param    model_name (str): Name of the pre-trained BERT model to use.\n",
        "        @param    freeze_bert (bool): Set to `True` to freeze BERT layers.\n",
        "        @param    hidden_size (int): Number of hidden units in the classifier layer.\n",
        "        @param    num_hidden_layers (int): Number of hidden layers in the classifier.\n",
        "        @param    dropout_prob (Optional[float]): Dropout probability for the classifier layer.\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.model_name = model_name\n",
        "        # Load the configuration\n",
        "        self.load_config(model_name,label2id = label2id ,id2label = id2label )\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(model_name , config = self.config)\n",
        "\n",
        "        # Freeze BERT layers (from updating the weights parameters )if specified\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Define classifier layer\n",
        "        layers = []\n",
        "        # holds the number of hidden units in each layer of the BERT model to make it input to the coming layers.\n",
        "        in_features = self.bert.config.hidden_size\n",
        "\n",
        "        for i in range(num_hidden_layers):\n",
        "            #  linear transformation to the input tensor\n",
        "            linear_layer = nn.Linear(in_features, hidden_size)\n",
        "\n",
        "            # initialize weights to keep the variance of the activations and gradients roughly the same across different layers,\n",
        "            # which can help prevent vanishing or exploding gradients during training\n",
        "            nn.init.xavier_uniform_(linear_layer.weight)\n",
        "\n",
        "            # Naming the parameters\n",
        "            # linear_layer.weight.names = ('in_features', 'out_features')\n",
        "            # linear_layer.bias.names = ('out_features',)\n",
        "\n",
        "            layers.append(linear_layer)\n",
        "\n",
        "            #  non-linearity into the model\n",
        "            if i % 5 == 0:\n",
        "              layers.append(nn.LeakyReLU())\n",
        "            elif i % 5 == 1:\n",
        "              layers.append(nn.GELU())\n",
        "            elif i % 5 == 2:\n",
        "              layers.append(nn.ELU())\n",
        "            elif i % 5 == 3:\n",
        "              layers.append(nn.Softmax())\n",
        "\n",
        "            layers.append(nn.LayerNorm(hidden_size))\n",
        "            if  i% 2 == 0 and dropout_prob is not None:\n",
        "              # reduces the risk of over-reliance on specific features that may not generalize well\n",
        "              # randomly sets a fraction of its input units to 0 during each training epoch\n",
        "                layers.append(nn.Dropout(dropout_prob))\n",
        "\n",
        "                dropout_prob = dropout_prob * 2\n",
        "\n",
        "            # in features : 0=>bert 1=>256 2=>128 3=>64\n",
        "            in_features = hidden_size\n",
        "            # hidden sizes : 0=>256 1=>128 2=>64 3=>32\n",
        "            hidden_size = hidden_size // 2\n",
        "\n",
        "\n",
        "        # we have 3 classes\n",
        "        # in features :  3=>64\n",
        "        num_classes = len(label2id) # = 3\n",
        "        layers.append(nn.Linear(in_features, num_classes))\n",
        "        self.classifier = nn.Sequential(*layers)\n",
        "    def load_config(self, model_name:str , id2label : Dict[int,str],label2id : Dict[str,int]):\n",
        "      self.config = AutoConfig.from_pretrained(model_name, num_labels = 3)\n",
        "      self.config.id2label = id2label\n",
        "      self.config.label2id = label2id\n",
        "\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        # outputs[0] = (batch_size, sequence_length, hidden_size)\n",
        "        # [:, 0, :] = selects all elements along the first dimension (batch size), the first element along the second dimension (which corresponds to the CLS token in BERT), and all elements along the third dimension (hidden size)\n",
        "        # final shape = (batch_size, hidden_size)\n",
        "        # [CLS] token is usually used to represent the entire input sequence and used for classification tasks\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOShh5khbbGX"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "def save_architecture_design(save_dir:str,model_name:str ,model):\n",
        "  if save_dir is not None:\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "  with open(os.path.join(save_dir,'architecture.txt'), 'w') as file:\n",
        "    print(f'{model_name} \\n' + f'epochs : {EPOCHS}  \\t Batch Size : {BATCH_SIZE} \\n' +str(summary(model)),file = file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiEwyv8ZAMYd"
      },
      "outputs": [],
      "source": [
        "def initialize_model(save_dir:str,model_name: str) -> Tuple[BertClassifier,torch.device]:\n",
        "    \"\"\"\n",
        "    Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): model checkpoint from huggingface\n",
        "\n",
        "    Returns:\n",
        "        Tuple[BertClassifier, AdamW, get_linear_schedule_with_warmup]: A tuple containing the BertClassifier object,\n",
        "        the AdamW optimizer object, and the learning rate scheduler object.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    model = BertClassifier(model_name,freeze_bert=False, dropout_prob = 0.25)\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
        "        # for parallelizing the training of a neural network across multiple GPUs\n",
        "        model = torch.nn.DataParallel(model)\n",
        "    # If there's a GPU available...\n",
        "    if torch.cuda.is_available():\n",
        "\n",
        "        # Tell PyTorch to use the GPU.\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "        !nvidia-smi\n",
        "\n",
        "    # If not...\n",
        "    else:\n",
        "        print('No GPU available, using the CPU instead.')\n",
        "        device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    save_architecture_design(save_dir,model_name,model)\n",
        "    return model , device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKvWV95AGL4c"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module, val_dataloader: DataLoader , loss_fn = None) -> Tuple[float, float, float, float, float, float]:\n",
        "    \"\"\"\n",
        "    After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set for a PyTorch model.\n",
        "     Args:\n",
        "        model: A PyTorch model.\n",
        "        val_dataloader: A PyTorch DataLoader for the validation set.\n",
        "        loss_fn : it is the loss function\n",
        "    Returns:\n",
        "        A tuple of floats with the average loss, accuracy, f1_score (weighted and macro), precision,\n",
        "        and recall on the validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during the evaluation time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "    val_f1_weighted = []\n",
        "    val_f1_macro = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # For each batch in our validation set...\n",
        "      for batch in val_dataloader:\n",
        "          # Load batch to GPU\n",
        "          b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "          # Compute the logits\n",
        "          logits = model(b_input_ids, b_attn_mask)\n",
        "          if model.model_name == 'AraBERT':\n",
        "            logits=logits.logits\n",
        "\n",
        "\n",
        "          # Compute the loss\n",
        "          loss = loss_fn(logits, b_labels)\n",
        "          val_loss.append(loss.item())\n",
        "\n",
        "          # Get the predictions\n",
        "          preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "          # Calculate the accuracy rate\n",
        "          accuracy = (preds == b_labels).cpu().numpy().mean()\n",
        "          val_accuracy.append(accuracy)\n",
        "\n",
        "          # Calculate the f1 score (weighted and macro), precision, and recall\n",
        "          val_f1_weighted.append(f1_score(b_labels.cpu(), preds.cpu(), average='weighted'))\n",
        "          val_f1_macro.append(f1_score(b_labels.cpu(), preds.cpu(), average='macro'))\n",
        "          val_precision.append(precision_score(b_labels.cpu(), preds.cpu(), average='weighted'))\n",
        "          val_recall.append(recall_score(b_labels.cpu(), preds.cpu(), average='weighted'))\n",
        "\n",
        "\n",
        "    # Compute the average accuracy, loss, f1 score (weighted and macro), precision, and recall over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "    val_f1_weighted = np.mean(val_f1_weighted)\n",
        "    val_f1_macro = np.mean(val_f1_macro)\n",
        "    val_precision = np.mean(val_precision)\n",
        "    val_recall = np.mean(val_recall)\n",
        "    return val_loss, val_accuracy, val_f1_weighted, val_f1_macro, val_precision, val_recall\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhUDEk_tg074"
      },
      "outputs": [],
      "source": [
        "def evaluate_test(model: nn.Module, test_dataloader: DataLoader ,save_dir:str,device ) -> pd.DataFrame:\n",
        "  loss_fn = nn.CrossEntropyLoss(weight = torch.tensor([1.0,1.0,1.0]).to(device))\n",
        "\n",
        "  test_loss, accuracy, f1_weighted, f1_macro, precision, recall = evaluate(model, test_dataloader,loss_fn)\n",
        "  results = [{'loss':test_loss,\n",
        "                        'accuracy':accuracy,\n",
        "                        'f1_weighted':f1_weighted,\n",
        "                        'f1_macro':f1_macro,\n",
        "                        'precision':precision,\n",
        "                        'recall':recall,\n",
        "                                  }]\n",
        "  results_df = pd.DataFrame(results)\n",
        "  if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "  model_name = model.model_name\n",
        "  if len(model_name.split('/')) == 2:\n",
        "    model_name = model_name.split('/')[1]\n",
        "\n",
        "  results_path = os.path.join(save_dir, f\"{model_name}.csv\")\n",
        "\n",
        "  # Print performance over the entire training data\n",
        "  print(results_df.head(EPOCHS))\n",
        "  # save the results into csv\n",
        "\n",
        "  results_df.to_csv(results_path)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvdB2C3xCkyx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "#from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm as ProgressBar\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          train_dataloader: DataLoader,\n",
        "          save_dir:str ,\n",
        "          eval_dataloader: DataLoader,\n",
        "          early_stopping_patience:int = 4,\n",
        "          epochs: int = 4,\n",
        "          log_steps:int = 1000,\n",
        "          checkpoint : Optional[str] = None,\n",
        "          device : torch.device = torch.device(\"cpu\"),\n",
        "          evaluation: bool = False,\n",
        "          ) -> None:\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Specifying Loss Function\n",
        "    loss_fn = nn.CrossEntropyLoss(weight = torch.tensor([1.0,1.0,1.0]).to(device))\n",
        "\n",
        "    # Create the optimizer\n",
        "    # To update only the trainable parameters\n",
        "    optimizer = AdamW(params = [parameter for parameter in list(model.parameters()) if parameter.requires_grad],\n",
        "                      lr=LEARNING_RATE,    # Default learning rate\n",
        "                      eps=EPSILON,    # Default epsilon value : value added to the denominator of the AdamW update to improve numerical stability\n",
        "                      betas=(0.9, 0.999) # Default betas value : exponential decay rates for the first and second moments of the gradients\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f'total steps = {len(train_dataloader)} batches * {epochs} epochs = {total_steps} \\t Batch Size {BATCH_SIZE}')\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    # Progress bar for tracking progress\n",
        "    progress_bar = ProgressBar(range(total_steps))\n",
        "\n",
        "    if save_dir is not None:\n",
        "      os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint_epoch = 0\n",
        "    if checkpoint is not None:\n",
        "      import pickle\n",
        "      checkpoint_path = os.path.join(save_dir,checkpoint)\n",
        "      #model_checkpoint_file = open(filename, 'rb')\n",
        "      print(f'Loading model from this checkpoint {checkpoint_path}')\n",
        "      #model.load_state_dict(torch.load(filename))\n",
        "      # load the checkpoint\n",
        "      checkpoint = torch.load(checkpoint_path,map_location=device)\n",
        "      model.load_state_dict(checkpoint['model_state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "      checkpoint_epoch = checkpoint['epoch'] + 1\n",
        "      loss = checkpoint['loss']\n",
        "      print(f'loaded checkpoint of epoch {checkpoint_epoch} successfully')\n",
        "      progress_bar.update(len(train_dataloader) * checkpoint_epoch)\n",
        "\n",
        "    # Trainable parameters\n",
        "    model_parameters = [parameter for parameter in list(model.parameters())  if parameter.requires_grad]\n",
        "\n",
        "\n",
        "    best_val_accuracy = 0.0 # initialize best validation accuracy to 0.0\n",
        "    best_model_state_dict = None\n",
        "    best_epoch = 0\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs-checkpoint_epoch):\n",
        "        epoch_i+=checkpoint_epoch\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits of shape (batch_size, num_classes)\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            # extract the scalar value of the loss from the tensor and accumulate the loss values\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\" (gradients become very large)\n",
        "            torch.nn.utils.clip_grad_norm_(model_parameters,\n",
        "                                           max_norm = 1.0,\n",
        "                                           norm_type = 2 # euclidian normalization square root sum of squares\n",
        "                                           )\n",
        "\n",
        "            # updates the model parameters using the computed gradients\n",
        "            optimizer.step()\n",
        "            # updates the learning rate based on the current epoch\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every log_steps batches and the last batch\n",
        "            if (step % log_steps == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:# After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy, val_f1_weighted, val_f1_macro, val_precision, val_recall = evaluate(model, eval_dataloader,loss_fn=loss_fn)\n",
        "            results = results + [{'epoch': epoch_i ,\n",
        "                                  'avg_train_loss':avg_train_loss,\n",
        "                                  'val_loss':val_loss,\n",
        "                                  'val_accuracy':val_accuracy,\n",
        "                                  'val_f1_weighted':val_f1_weighted,\n",
        "                                  'val_f1_macro':val_f1_macro,\n",
        "                                  'val_precision':val_precision,\n",
        "                                  'val_recall':val_recall,\n",
        "                                  \"time elapsed\":time.time() - t0_epoch\n",
        "                                  }]\n",
        "            results_df = pd.DataFrame(results)\n",
        "            results_path = os.path.join(save_dir, f\"epochs{epoch_i}-results.csv\")\n",
        "            # Print performance over the entire training data\n",
        "            print(results_df.head(EPOCHS))\n",
        "            # save the results into csv\n",
        "            results_df.to_csv(results_path)\n",
        "            print(\"-\"*70)\n",
        "            print(\"\\n\")\n",
        "            if(best_val_accuracy <= val_accuracy):\n",
        "              best_val_accuracy = val_accuracy\n",
        "              best_epoch = epoch_i\n",
        "              torch.save(model.state_dict(), os.path.join(save_dir,\"best_model.pth\"))\n",
        "              best_model_state_dict = model.state_dict()\n",
        "\n",
        "\n",
        "        if save_dir is not None: # saving the checkpoint after each epoch\n",
        "            checkpoint_path = os.path.join(save_dir, f\"checkpoint-epoch-{epoch_i+1}.pt\")\n",
        "            torch.save({\n",
        "            'epoch': epoch_i,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "        if (epoch_i - best_epoch) == early_stopping_patience: # early stopping if there is no improvment in the accuracy\n",
        "          print(f'Early stopping at epoch {epoch_i} best epoch is {best_epoch}')\n",
        "          break\n",
        "\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    if best_model_state_dict is not None:\n",
        "      print(f\"loading the best model of epoch {best_epoch+1} based on it's best accuracy {best_val_accuracy}\")\n",
        "      model.load_state_dict(best_model_state_dict)\n",
        "\n",
        "\n",
        "\n",
        "    # Print performance over the entire training data\n",
        "    print(results_df.head(len(results)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tXKFO_y9_bS"
      },
      "source": [
        "### Evaluation - Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfBbIxW1FTat"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(f\"{os.path.join(PREPROCESSED_MERGED_DATASET_DIR, 'unstemmed/'+ TEST_DATASET_NAME + '.csv' )}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPoyoj2I9IkA"
      },
      "outputs": [],
      "source": [
        "# df_test = dataset[\"test\"].to_pandas()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcwsUP3L-GeZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Tuple, List\n",
        "\n",
        "from tqdm.auto import tqdm as ProgressBar\n",
        "\n",
        "\n",
        "def bert_predict(model: nn.Module, test_dataloader: DataLoader,device) -> Tuple[List[float],List[float]]:\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained BERT model.\n",
        "        test_dataloader (DataLoader): The test data loader.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: The predicted probabilities for each class.\n",
        "    \"\"\"\n",
        "  # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "    # Progress bar for tracking progress\n",
        "    progress_bar = ProgressBar(range(len(test_dataloader)))\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(sample.to(device) for sample in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "\n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "    tensor = torch.from_numpy(probs)\n",
        "    ytest = torch.max(tensor, dim=1)\n",
        "    return tensor,ytest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2owVmHlPH5yA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from typing import Tuple, List\n",
        "\n",
        "from tqdm.auto import tqdm as ProgressBar\n",
        "\n",
        "\n",
        "def ensemble_bert_predict(models: List[nn.Module], test_dataloader: DataLoader, device,save_dir:str, contribution_percentages: Optional[List[float]] = None,is_accuracy:Optional[bool]=True) -> Tuple[List[float], List[float],pd.DataFrame,List[float]]:\n",
        "    \"\"\"Perform a forward pass on a list of trained BERT models to predict probabilities\n",
        "    on the test set and ensemble the predictions using weighted averaging based on contribution percentages.\n",
        "\n",
        "    Args:\n",
        "        models (List[nn.Module]): The list of trained BERT models.\n",
        "        test_dataloader (DataLoader): The test data loader.\n",
        "        device: The device to perform the inference on (e.g. 'cpu' or 'cuda').\n",
        "        contribution_percentages (List[float], optional): The contribution percentages for each model in the ensemble.\n",
        "            This should be a list of floats that sum up to 1, representing the weights of each model in the ensemble.\n",
        "            If not provided, equal contribution percentages will be used for all models. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[float], List[float]]: The ensembled predicted probabilities for each class.\n",
        "    \"\"\"\n",
        "    df_path=os.path.join(save_dir,\"Models'_Accuracies.csv\")\n",
        "    if os.path.exists(df_path):\n",
        "      print('successfully loaded accuracies')\n",
        "      df= pd.read_csv(df_path)\n",
        "      contribution_percentages = df['contribution_percentage'].tolist()\n",
        "      accuracies = df['accuracy'].tolist()\n",
        "\n",
        "\n",
        "    if (contribution_percentages is None or len(models) != len(contribution_percentages)) and is_accuracy:\n",
        "        # Calculate contribution percentages based on accuracy of each model\n",
        "        accuracies = []\n",
        "\n",
        "\n",
        "        print('calculating accuracies ...')\n",
        "        for model in models:\n",
        "            # Put the model into the evaluation mode. The dropout layers are disabled during the test time.\n",
        "            model.eval()\n",
        "\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Progress bar for tracking progress\n",
        "            progress_bar = ProgressBar(range(len(test_dataloader)))\n",
        "\n",
        "            # For each batch in our test set...\n",
        "            for batch in test_dataloader:\n",
        "                # Load batch to device\n",
        "                b_input_ids, b_attn_mask = tuple(sample.to(device) for sample in batch)[:2]\n",
        "\n",
        "                # Compute logits\n",
        "                with torch.no_grad():\n",
        "                    logits = model(b_input_ids, b_attn_mask)\n",
        "                 # Calculate accuracy\n",
        "                if model.model_name == 'AraBERT':\n",
        "                  predicted = torch.argmax(logits.logits, 1)\n",
        "                else:\n",
        "                  _, predicted = torch.max(logits.data, 1)\n",
        "                total += b_input_ids.size(0)\n",
        "                correct += (predicted == batch[-1].to(device)).sum().item()\n",
        "                progress_bar.update(1)\n",
        "\n",
        "            accuracy = correct / total\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "        # Normalize accuracies to get contribution percentages\n",
        "        total_accuracy = sum(accuracies)\n",
        "        contribution_percentages = [round(accuracy / total_accuracy,2) for accuracy in accuracies]\n",
        "        print(contribution_percentages)\n",
        "    elif contribution_percentages is None and not is_accuracy:\n",
        "      contribution_percentages = [round(1/len(models),2)] * len(models)\n",
        "\n",
        "    assert len(models) == len(contribution_percentages), \"Number of models must be equal to number of contribution percentages.\"\n",
        "    #assert sum(contribution_percentages) == 1, \"Contribution percentages must sum up to 1.\"\n",
        "    if save_dir:\n",
        "      data = []\n",
        "      for i,contribution_percentage in enumerate(contribution_percentages):\n",
        "        data =data + [\n",
        "                {'name':models[i].model_name,\n",
        "                'contribution_percentage':contribution_percentage,\n",
        "                 'accuracy':accuracies[i] if is_accuracy else contribution_percentage,\n",
        "                }\n",
        "                  ]\n",
        "      df = pd.DataFrame(data)\n",
        "      df.to_csv(df_path)\n",
        "      print(f\"Saved Models' Accuracies in {df_path}\")\n",
        "\n",
        "    # Put the models into the evaluation mode. The dropout layers are disabled during the test time.\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "    # Progress bar for tracking progress\n",
        "    progress_bar = ProgressBar(range(len(test_dataloader)))\n",
        "    print('computing logits ...')\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to device (to extract the first two elements from the tuple, which correspond to b_input_ids and b_attn_mask)\n",
        "        b_input_ids, b_attn_mask = tuple(sample.to(device) for sample in batch)[:2]\n",
        "\n",
        "        # Compute logits for each model\n",
        "        batch_logits = []\n",
        "        for i, model in enumerate(models):\n",
        "            with torch.no_grad():\n",
        "                logits = model(b_input_ids, b_attn_mask)\n",
        "                if model.model_name == 'AraBERT':\n",
        "                  logits = logits.logits\n",
        "\n",
        "            #batch_logits.append(contribution_percentages[i] * F.softmax(logits, dim=1))  # Weighted contribution of each model's logits softmaxed\n",
        "            batch_logits.append(contribution_percentages[i] * torch.tanh(logits))  # Weighted contribution of each model's logits with Tanh normalization\n",
        "\n",
        "        # Average logits across models\n",
        "        avg_logits = sum(batch_logits)\n",
        "        all_logits.append(avg_logits)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "    tensor = torch.from_numpy(probs)\n",
        "    ytest = torch.max(tensor, dim=1)\n",
        "\n",
        "    return tensor, ytest, df,contribution_percentages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Crx0sNxxxhYE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import List,Tuple\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "def predict_sentiment(models: List[nn.Module], text: str, tokenizer: BertTokenizer, save_dir:str, device) -> Tuple[str,pd.DataFrame]:\n",
        "    input_ids, attention_masks = preprocessing_for_bert(tokenizer, [text], text_preprocessing_fn=None)\n",
        "    # Put the models into evaluation mode. The dropout layers are disabled during the test time.\n",
        "    class_names = ['positive', 'negative', 'neutral']\n",
        "\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "        model.to(device)  # Load model onto the specified device\n",
        "\n",
        "    df_path = os.path.join(save_dir, \"Models'_Accuracies.csv\")\n",
        "    if os.path.exists(df_path):\n",
        "        print('successfully loaded accuracies')\n",
        "        df = pd.read_csv(df_path)\n",
        "        contribution_percentages = df['contribution_percentage'].tolist()\n",
        "        accuracies = df['accuracy'].tolist()\n",
        "    else:\n",
        "      print(\"Models'_Accuracies.csv is needed !!!\")\n",
        "\n",
        "    all_logits = []\n",
        "    model_answers = []  # List to store each model's answers\n",
        "\n",
        "    for i, model in enumerate(models):\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids.to(device), attention_mask=attention_masks.to(device))[0]  # Assuming the model takes input_ids and attention_mask as input\n",
        "        all_logits.append(contribution_percentages[i] * logits)  # Weighted contribution of each model's logits\n",
        "         # Convert logits to predicted sentiment for the current model\n",
        "        predicted_sentiment = torch.argmax(logits)\n",
        "        model_answers = model_answers + [class_names[predicted_sentiment]]\n",
        "\n",
        "    # Aggregate the logits from all models\n",
        "    aggregated_logits = torch.stack(all_logits).sum(dim=0)\n",
        "\n",
        "    # Convert logits to predicted sentiment\n",
        "    predicted_sentiment = torch.argmax(aggregated_logits)\n",
        "\n",
        "    # Map the predicted sentiment index to corresponding sentiment label\n",
        "    predicted_sentiment_label = class_names[predicted_sentiment]\n",
        "\n",
        "\n",
        "    # concatenate model answers\n",
        "    df['model_answers'] = model_answers\n",
        "\n",
        "    df_sorted = df.sort_values(by='accuracy', ascending=False)\n",
        "\n",
        "\n",
        "    return predicted_sentiment_label,df_sorted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-zZvv_g_dG8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def print_classification_report(y_pred:torch.tensor,save_dir:str,title:str):\n",
        "    global df_test\n",
        "    y_true = df_test['LABEL']\n",
        "    y_pred = y_pred.indices\n",
        "\n",
        "\n",
        "    class_names = ['positive', 'negative', 'neutral']\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    # Create a heatmap of the classification report\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    im = ax.imshow(report_df.iloc[:-1, :-1], cmap='YlGnBu')\n",
        "    report_df.to_csv(os.path.join(save_dir,title+\".csv\"))\n",
        "\n",
        "    # Set axis labels and ticks\n",
        "    ax.set_xticks(range(len(class_names)))\n",
        "    ax.set_yticks(range(len(report_df.index[:-1])))\n",
        "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
        "    ax.set_yticklabels(report_df.index[:-1])\n",
        "\n",
        "    # Add text labels to the heatmap\n",
        "    for i in range(len(report_df.index[:-1])):\n",
        "        for j in range(len(class_names)):\n",
        "            ax.text(j, i, \"{:.2f}\".format(report_df.iloc[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
        "    # Add text labels to the heatmap\n",
        "    # for i in range(len(report_df.index[:-1])):\n",
        "    #     for j in range(len(class_names)):\n",
        "    #         precision = report_df.iloc[i]['precision']\n",
        "    #         recall = report_df.iloc[i]['recall']\n",
        "    #         f1_score = report_df.iloc[i]['f1-score']\n",
        "    #         ax.text(j, i, \"{:.2f}\\n{:.2f}\\n{:.2f}\".format(precision, recall, f1_score), ha=\"center\", va=\"center\", color=\"black\")\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = ax.figure.colorbar(im, ax=ax)\n",
        "    cbar.ax.set_ylabel('Score', rotation=-90, va=\"bottom\")\n",
        "\n",
        "    # Save the classification report as an image\n",
        "    plt.tight_layout()\n",
        "    plt.title(title)\n",
        "    plt.savefig(os.path.join(save_dir,title),format='png')\n",
        "    plt.show()\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC9RRiTc-9Rv"
      },
      "outputs": [],
      "source": [
        "def show_confusion_matrix(y_pred:torch.tensor,title:str,save_dir:Optional[str] = None):\n",
        "  global df_test\n",
        "  y_true = df_test['LABEL']\n",
        "  y_pred = y_pred.indices\n",
        "\n",
        "  class_names = ['positive', 'negative', 'neutral']\n",
        "  cm = confusion_matrix(y_true,  y_pred)\n",
        "  df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "  hmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"BuPu\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment')\n",
        "  plt.title(title)\n",
        "  plt.tight_layout()\n",
        "\n",
        "  if save_dir is not None:\n",
        "    plt.savefig(os.path.join(save_dir,title),format='png')\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSNujt8YE6IN"
      },
      "source": [
        "### CAMEL BERT - (DA)\n",
        "\n",
        "> pretrained on Dialectel only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQbj_VwZWI16"
      },
      "outputs": [],
      "source": [
        "CAMEL_DA_model_huggingface_checkpoint='CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75zNmfSyf_W5"
      },
      "source": [
        "#### Camel-BERT Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "ab74d9dd1ee04d89a3b74801bebe98bd",
            "73a13d28915d414cb80ea392725d0400",
            "4e195afc577b45fca967123b5b05b367",
            "681a089f88f74ff3b8af56f56fdfe004",
            "c51066eb13d7446aa02310eefeab3de7",
            "5fa4274d36e240c687d0e90cfbdff019",
            "2b7cb8f66ac1424b84d7635c267222df",
            "9be1c25d35484c6da8e9fb245f962055",
            "6c98965fa7134fa182b7e79fc54b32ef",
            "6ce86f05db164a67a53e4d884eeb6825",
            "89278f2dcf2f4b28a07d687c6957479a",
            "ce54e44f809e4677b366ec6f1815efa5",
            "8b2bbd1345a34e79970c173c890edfb7",
            "5054ad837b3a43a8bb3711458ba5341c",
            "52f8eaefddcb460988bb17e0c140086b",
            "ea1ff64d833f49ce926ed3114cbbe732",
            "deb23970b60144ea985cd77343c88688",
            "3f74f8e6f8b5495bbdae70b5eaa5fc26",
            "9d273a9f10de40c197a96218dee1ab75",
            "b390db42c21c45d88a4f4e51eb189603",
            "d611b4c26b0b4a26a0642c18853892d4",
            "20492d202a9f40d49c888c5094b73007",
            "6b7075a9441841abbe9c0976fc634990",
            "a97690fd75ec4130be22619d2a5597e0",
            "1afab39f29ad4534be9660e6f9d55a4e",
            "b513fecfcc7f44fda459411784250aa3",
            "cba2bad306f543f4b7be0e10caa8048e",
            "c292b652450c49308cd678f5e5fd2ccd",
            "c96aeba2167e43698f9bfe77bd4b4dc8",
            "e2b9d399fafd4ac0aabd3f491fbdd0a6",
            "9f3c8ec925db4933aa82d4d23b49f952",
            "cf2d2c2d218f4cfdb349df0c6858e21b",
            "ff780f245bfc48b8b4cacfc579457354",
            "4d7e7f06451d4dc885a58ecb14681f2a",
            "853e617ec72f46ee828dda9eb9449687",
            "ba1d9245407a4a9d8caa20a55f2d19e1",
            "67d5282835d94a87bbe1b48ae748d4a8",
            "c556987f1a8144d28e6beaf5a5387998",
            "3ed7e52c74a2406ab4f010ce9838200e",
            "ff47da0423db44adbc2cecdeaea4304e",
            "fa2793970ddc4b6d99b4e8e4e0bd70f8",
            "2824ea891d9e420b8e3243a52ef92ad6",
            "79cb74640e514a2d8a4248cc8ce951fa",
            "c9cf0146cf6844b79a4ad6deaacc4726"
          ]
        },
        "id": "783oyk54f_W6",
        "outputId": "f836cea5-5b69-485f-ab9f-2e66d7eca005"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CAMEL_DA_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "959d9c150a9f47f7938b3001c7e6f95d",
            "8560b2262a054007b1657a208f5b7174",
            "c8ea15301dd441659ca10abf03525f10",
            "1adcfdc2883545b6860a957bab018e9d",
            "c5047eb2bd4a4777ad449f49f91e32de",
            "6f90d234567d49178dca24d4a37614c8",
            "8a45338ba97243e5b5d8f741f69f69cd",
            "ce64ea83d47b42fc904fa9cacebcaab4",
            "d0876172c339440b8671b5487a26b7ef",
            "860b745016104232a5496fb0e8ae441e",
            "3f2fb1fe0b2a4495baf26522e91c68cb",
            "7b563965ec2f4de789db2e6161c5ac0e",
            "d9070430311a4025899767dff9913a41",
            "e9032cfab4474b6ebcc5b21ccd397f04",
            "c4bb4ef2258b466286450e8f069bc8ac",
            "3ce4d5b1214645319fe9bf3e59c69923",
            "0d507f80cd4c4ea2b81b5f408afaa5dd",
            "db1ebdc196144379994822c16bed2f22",
            "14362c2a7fce4c98afd5379440c369f4",
            "2b1b37f41d4541e8b4fc130be723ecd8",
            "23e08f83455845948a013c55825459ad",
            "238944c94a2a43979ec6264a9c367f5b",
            "89ea29eb2e9f404d9e7418e81af0ef7e",
            "80cdd7874ed949fe8c3201f3d719a60e",
            "f549f16bae464fee94a0eee7a6e4e4b0",
            "a474cd71ae584f908f637429097fb793",
            "1a27894ff1a44e0486508ccd2df25dad",
            "9db5f9ced62b41bb9a522f5f8c77ea6c",
            "2550ab443d754fcba71ce8741af4fdc0",
            "0df945388fae43329da84f42c08d2943",
            "e4ac337c1ed74b1ca33901b45ae10e40",
            "6c0dd492e4e14aa39a197fc673d10f6e",
            "baee1aac80d349c7a9647c3690dfb335"
          ]
        },
        "id": "HOvFNAlkGdLy",
        "outputId": "a87cc0f5-fee8-4ae8-b769-fae1fa58cb59"
      },
      "outputs": [],
      "source": [
        "PY_DATA_COLLATOR = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQDKe5kShgDH"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pDVBVndske1",
        "outputId": "9d8b8de5-3536-4c3c-ac6c-a6383cc1b1f8"
      },
      "outputs": [],
      "source": [
        "# The most recent checkpoint file will be listed at the top.\n",
        "!ls -lt {PY_CUSTOM_CAMEL_MODEL_DIR}checkpoint-*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463,
          "referenced_widgets": [
            "9c2e0dc6003141c1aad5bce832e95705",
            "f31ad4f8354540d984acd7bd5d9013a6",
            "5ee7ae88d0cf45899815e38d4dcaefce",
            "d6a9d97e15654b06a051ba85dbfd9c88",
            "ae703ef740064fdaa79271cfaa100f96",
            "7558f790a86341b990a91f3018be39ce",
            "85771a6cce3d4a34a471cf55b77d4205",
            "bb9e0f262e294a90b65c29d5f6ac1240",
            "c869ea29a07f444eabecca26d41abb27",
            "19759411c54749319a8a67e42738af78",
            "361d3a4e9d6e4363beb1ca146c521969",
            "496bb76865ce4a9caf32fbbd486d245f",
            "b1b090f04fbe4249a7aa534d2ac99f69",
            "b7b3ad63b8834e6ebcf5663057be5ac8",
            "6fc4bc56ddbd470ea395732873fe901b",
            "85e62164c5b2413e843d190c18314303",
            "68fd8f1bbc364ff29686c53e30c515f4",
            "4f6d989acc674420a0d67747a09c5b71",
            "d11825505113447da040dbe95e707d86",
            "a51ce82c14c1497a8f9cc22352d2202a",
            "d86fe2b235144496bcc072a1249e2f4d",
            "a10d61ab7fff4cc8a384b04761bb274f"
          ]
        },
        "id": "dmZWZbxdtVZs",
        "outputId": "677651c7-6b36-423d-9a6b-e235e1e04567"
      },
      "outputs": [],
      "source": [
        "set_seed(RANDOM_STATE)\n",
        "model , device = initialize_model(save_dir = PY_CUSTOM_CAMEL_MODEL_DIR,model_name = CAMEL_DA_model_huggingface_checkpoint )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAh2fbI7gP1Q",
        "outputId": "37cdde1f-e6b7-4f24-8624-c638e803c48b"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a6239f274f2642edb933b8e4323ce864",
            "a6f48993c6ea485ba6ff28b421533de3",
            "ad4f9115b40c4e2f80bd84fbcb53fcce",
            "b16994ac3afe41d398284a35b65245f4",
            "a432b8a555a64d938e552bb8eb367fef",
            "b2b62794ce664fc19a89946e7f70e23a",
            "eb9ad9409ea84db08b4aa3801361595c",
            "a2e69455963b4d0b92bd9abc3c044c47",
            "b8567990ed33401db35edc6831fcaba9",
            "5bd267eb456143c89fb8dc8b4e603a5c",
            "ae839433f820485eb8b7002494dfdb5b"
          ]
        },
        "id": "__wRcA0jK82n",
        "outputId": "2d649bb8-92b9-4bb3-9f69-5a56c93d604d"
      },
      "outputs": [],
      "source": [
        "train(model= model,\n",
        "      train_dataloader = train_dataloader,\n",
        "      eval_dataloader = eval_dataloader,\n",
        "      save_dir = PY_CUSTOM_CAMEL_MODEL_DIR,\n",
        "      device = device,\n",
        "      epochs = EPOCHS,\n",
        "      checkpoint = None,  # 'checkpoint-epoch-6.pt',\n",
        "      evaluation = True\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1KNNcQ_WJ7"
      },
      "source": [
        "#### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b4a3df6d9457473b93b2f033c2825af7",
            "f5a81c5c40554d82946b94d9ae34304e",
            "8436b5b1cdf843f0997844d933fc3a84",
            "36e9916b8cde4e84afce7869d4897927",
            "19ee8244bd9843ad98e6e2f85025d3e6",
            "469f09772f3b499499b0b905f7d7fecc",
            "88422232b4c049fbaa18b5e97bf35c72",
            "a999856b948348418d33ce095155ee80",
            "28d7d4237c4a487f87880e97c83cfe84",
            "b8b38211652d454d9ca0b7fa0244ed39",
            "57caa988c4ec4372bf1813276ef4bde8"
          ]
        },
        "id": "-UGBDKt2_WJ9",
        "outputId": "04c9fb38-84c5-42cf-8714-a3a8e135577c"
      },
      "outputs": [],
      "source": [
        "tensor , ytest = bert_predict(model, test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEP3Tx6X_WJ_"
      },
      "outputs": [],
      "source": [
        "# print_classification_report(ytest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "z2RI50-ROiWs",
        "outputId": "198e2c6f-df54-416e-8d91-bbc49a309be2"
      },
      "outputs": [],
      "source": [
        "print_classification_report(ytest, PY_CUSTOM_CAMEL_MODEL_DIR, 'Camel-da_classification_report')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "9N2IN1Qe_WKA",
        "outputId": "2981905d-cbf2-4ae9-c669-64144dd03d64"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(ytest,title = 'confusion matrix - CAMEL-DA',save_dir = PY_CUSTOM_CAMEL_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9pGQrzYcqnj"
      },
      "source": [
        "### CAMEL-BERT-MIX\n",
        "\n",
        "> This model card describes CAMeLBERT-Mix (bert-base-arabic-camelbert-mix), a model pre-trained on a mixture of these variants: MSA, DA, and CA.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0qPhD7bcqnk"
      },
      "outputs": [],
      "source": [
        "CAMEL_MIX_model_huggingface_checkpoint='CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XusZn9lKcqnl"
      },
      "source": [
        "#### CAMEL-BERT-MIX Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "c10ee1edaef34145bb50d0843981468b",
            "e43b4864a0e14475bf0ce2e5b59d3d9c",
            "a5ee6b1566bb49b2b564078693e133bb",
            "e699253697db4a6f977a82c28afd9402",
            "2f581523d6d742f1af1c05b88499c1d4",
            "2c6669be5b9c47a49a9f21c853a111de",
            "8cb2ae7d11104ecdb707594144985508",
            "bc6ff246258b4fa389fb63f3c44ac0c1",
            "b2b6484a7509470ca4cbd4aacdfc95e6",
            "55768973e0894fa9abdcc39689e8ad2b",
            "a9a93fbc194c42bf9ed0cb6d4ef071a2",
            "14458c46c2804b91b0d1bf57167e8d16",
            "75d81629f70746efb0012c415f9a3340",
            "8717a330bc524e9db5bae07a876dce7c",
            "60cc60682d984d2c924ef87ca8519632",
            "5a6dfcf2086e42e082bcdd37c3e60f0f",
            "8fd5d95bd5514a1ab1aa071893c5596c",
            "3cc0e9dbdac144c9b0a17f3bb6b89c14",
            "766778dd0da4472b81480831b40da960",
            "a2fbbf95afa243deb342438c619441fd",
            "3405bff80b124dedb9e4c17e3b52ad9a",
            "1d4b70399e664d90981d700cf8b286be",
            "3904fbe2c7154c558e5309b7c91fa568",
            "9dec322788bd4e5198576288174e7b99",
            "14efdf68779c47dda0f33084bc79067d",
            "a81e8f4d3a794fb6ab17a4d08c885ffc",
            "971cbfffede64e46ad0cfa48ee4468ac",
            "75b6cc63515b459e9592ae1bd1c083f9",
            "2b2b2edd6bd94673a3e20d513a9cfc6c",
            "ca428421d53c445785fa20376f46b39e",
            "68ee66c49c2b435c95740c1390754eb0",
            "68ecb0eb88dc485189d017e0aef81b72",
            "07f19612477646f99158ea4ca23b7d1d",
            "c2e3a562bb904d98a5da35eb0f083bd6",
            "7631e79655c049dfabfd2841c909a7ab",
            "468895a9fceb4f43bc8d88d529bcaa3e",
            "df73de8eea014c7b882eb07b2f5bd389",
            "c0cdc490a1f64cce836156f76b9a70b1",
            "6d00895743f949f0a0851076c3260853",
            "f4ebce37aaed43089598fc33fcd7c42d",
            "b855cfc8d7e14653a9e58636d66a4b5b",
            "30ffccbd17f54102a4b36bf3df3dc04f",
            "f99781c1300144f187e14f882dd05851",
            "134b9df67b0f4d71a2de4fe6b775714c"
          ]
        },
        "id": "3h_gWtv4cqnl",
        "outputId": "36ce4b74-21be-4896-f07a-2d100e0d396b"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CAMEL_MIX_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "fbe1e28a5b154e50a7cf158cdc636674",
            "af04951daa55417ca1b6468670fca060",
            "fd52dfddea054fcbb4e7f6b618468df8",
            "ea204fbe30814af192f51b794d26e880",
            "c9ebabaccba04b24ad75ea5a87805a43",
            "131a7e3f84cc4603b869c0c303a61355",
            "bf36ca844f804292addc00884997553e",
            "de4a4d14008345e59866f209535afb3a",
            "d084637fd6484066ab50bfcc4f4abb5e",
            "62e5becefc23432ea43887be9edec7eb",
            "d9b24f30973f4c4a91fb2e71a6e94de1",
            "b032931517dc4b38af2f838fbe8009fe",
            "fb0912e1ad854cbbad38bc887aaabf83",
            "e4d11579194641c28fe1ee329ed5b84a",
            "4ed8dffd2c8c4917873567d04aa114c9",
            "23c0cbca1de442be9696be4e53bbc6ef",
            "a328c010b09b4e418e43409d81051e4c",
            "db5819cf073a4927a287500454306b73",
            "d17678e5d37d4e8d9358b32f230b0211",
            "97f0f43e461f49fc81ef850a43e973df",
            "3334ec9e053b4e619c5dc465c9d86041",
            "513d0290619242649e662a212be47c4f",
            "56dfd81eb31540fe94c9f33e4cc16f62",
            "11acc71e0ddf4379a2e5ae373d18d174",
            "d0a0052b7eda436eb1d69089c9cecc37",
            "a2b12c0fcbab4ef7899b4719a7f0fc68",
            "04158a728483451eae528c91508ee12a",
            "4d66429cd0f7447f9a3cad0abb9c1420",
            "75cd5abfd7fd45ce89d6f096a063581e",
            "165dcb8d455b40018d4ac8fd1e49377b",
            "00208e52d99e4c7aaa087a8239779b54",
            "838e133f0e0f4efeb75499eba4ea5df3",
            "d167775b99654f0580b4ad257f753b00"
          ]
        },
        "id": "-TVzTmikcqnm",
        "outputId": "b7ef3664-8c63-4e64-f1f3-bc51e6d34bb5"
      },
      "outputs": [],
      "source": [
        "PY_DATA_COLLATOR = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZixI4kUcqnn"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnUZrANGcqnn",
        "outputId": "b5dec033-b920-4878-c7dc-beb7b5cc03a7"
      },
      "outputs": [],
      "source": [
        "# The most recent checkpoint file will be listed at the top.\n",
        "!ls -lt {PY_CUSTOM_CAMEL_MIX_MODEL_DIR}checkpoint-*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "d14fbede096f47478c3a0e42319098b3",
            "7358ff878c494f309b121e205978c9bb",
            "7ec80629e5ef4e0a9c3a110229e38b00",
            "e2bf1f2d7f964eef8f8267ac92f36870",
            "ee68c24fc10140d2af089a090243a5dc",
            "f35a0cf2e625401196dcaa8e2d6b729d",
            "76a2431660724a47a4b7e6b33302a0b1",
            "6fa84580597c46cda7d80c531ed5c281",
            "98c4889b71cf483d8d04c06c149071ec",
            "6afd47ca87c64e648f792118bc34a200",
            "3d0d7e94c4314fbeba02e1847770d403",
            "83c9b1ca9c1448588ea6aaa20e5c97d6",
            "c4dde66b39164b6c8347e2f6b006ec44",
            "460597eeaac640c88ba65a34d0b5d291",
            "7d90463154c34716a0ada6dc348f955b",
            "13d2670a178649d7a2bccd4a50bb9c61",
            "a94d7d4c47614a1394a270016124e40a",
            "6197c599d86c4821a7c4614e53d77af9",
            "0c39dc6f12ae4ac3a3a5b73c47d7221c",
            "d8feb049d301403d896f290d78d146f1",
            "2e2de59957424ae380eab8311428eba9",
            "eb135f6e51ef45e2bb8de8f2e461d46a"
          ]
        },
        "id": "q59YRQQccqno",
        "outputId": "6d3252ac-0c1b-4301-8642-f7e4db1cc9af"
      },
      "outputs": [],
      "source": [
        "set_seed(RANDOM_STATE)\n",
        "model , device = initialize_model(save_dir = PY_CUSTOM_CAMEL_MIX_MODEL_DIR,model_name = CAMEL_MIX_model_huggingface_checkpoint )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLahfFdscqnp",
        "outputId": "1a588ea7-ee48-46d5-e98b-41d86f16cb1f"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4c7cb1332d3e4aab8c00efb91ff57147",
            "e23a61f293a442aebab3faf72787430e",
            "cdabf0945d64426497bd0dc15be593d3",
            "850c4b79807e4ffaa12920f1c5e06d0a",
            "175b73ab4fa74b939ba73dbcc075d88e",
            "194e05a1b91847b18c575ea2f68666c9",
            "ae2ad4e386544f22ae89a506f5be9df2",
            "cfa90eaf82e94a9a9b36fdb0a2aa1ebc",
            "bc01364033cf4c958fdf100bd1eab1ff",
            "d06e5be2dc8b42f39d13253691b12dc4",
            "29acc110023146fe9e0f3de10656be5a"
          ]
        },
        "id": "1vPfyNeZcqnp",
        "outputId": "ef19caff-6952-41c5-ff20-749166a464ea"
      },
      "outputs": [],
      "source": [
        "train(model= model,\n",
        "      train_dataloader = train_dataloader,\n",
        "      eval_dataloader = eval_dataloader,\n",
        "      save_dir = PY_CUSTOM_CAMEL_MIX_MODEL_DIR,\n",
        "      device = device,\n",
        "      epochs = EPOCHS,\n",
        "      checkpoint = None ,#'checkpoint-epoch-7.pt',\n",
        "      evaluation = True\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnJ5ywAYGe13"
      },
      "source": [
        "#### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "032ee785f871415596297f2d3fe6eda0",
            "41abb550735e48a994d19e165b70d224",
            "fbf895e5add44f9d92fec8ac0f2becd8",
            "ec3e14cbbdeb411880e7d0d65f033522",
            "fe23476e841547b58665892ef42d1c73",
            "cf5227646bf34185a3d0aac86547b450",
            "17c25896411d44a28cbb1392a00732f6",
            "16dff5a2e7b941a6b586ec08cd9a4b8d",
            "b67ec30512514094a24d81db6061c438",
            "f703d120212a479fb9b1f72a878f8a0e",
            "e9c4892b45ae43658d274d634c87f1be"
          ]
        },
        "id": "OXtbBujtEPRE",
        "outputId": "18f0d83a-4e1b-481c-d0d0-e76335c2d826"
      },
      "outputs": [],
      "source": [
        "tensor , ytest = bert_predict(model, test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "mn_WKrRFGj16",
        "outputId": "41cafd38-0461-48d8-d709-10bcee60ee49"
      },
      "outputs": [],
      "source": [
        "print_classification_report(ytest, PY_CUSTOM_CAMEL_MIX_MODEL_DIR, 'Camel-mix_classification_report')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN6T0QCGaCyN"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# def print_classification_report(y_pred, save_dir='./reports', title='Classification Report'):\n",
        "#   # ... (rest of your function code) ...\n",
        "\n",
        "#   # Create the directory if it doesn't exist\n",
        "#   os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#   report_df.to_csv(os.path.join(save_dir, title + \".csv\"))\n",
        "#   # ... (rest of your function code) ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "okdU3LQOG2Vs",
        "outputId": "c7059d19-d415-4c46-83fe-24d6f505791e"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(ytest,title = 'confusion matrix - CaMelBert Mix',save_dir=PY_CUSTOM_CAMEL_MIX_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hemuWBW7HZo"
      },
      "source": [
        "###qarib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LrgD1kr6xh_"
      },
      "outputs": [],
      "source": [
        "# qarib_model_huggingface_checkpoint='ahmedabdelali/bert-base-qarib'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "4e9e4bbd3e1d414ba42be10ab05e67ae",
            "d5e31ad32dd4488385b568b4eb4be610",
            "54d95bbc586b4499baff94aed6d4383f",
            "758f0b3f84a04afbab5d85dd8257ac1a",
            "71a6bd637ae0496caa784d3983c1eede",
            "dd741ce266eb454385c600b81d3328d9",
            "f119dc3f6bf647bfa5c5cc97bc242641",
            "0fb07c92b9b24ba9880db4b2897694db",
            "3cfcf6f7f40340b3939eb0c885139bef",
            "d13cd53cca1f4eb7bf8eba167dd66bcf",
            "67c7d0c403f74515b36a466065cfd2b9",
            "4a5f61df65d54188a933a66d69ca141a",
            "9873eff0da644e72b08953159f1e99aa",
            "cc0a62c3484741a1b7240e42158b9ede",
            "a67857149d0b46bfa3ba8bdfd93cdeac",
            "407c2a02664a4b63bf007412366dca6f",
            "1d3d2dd8add940c7974eb393af6ace54",
            "07f3062be5754e268fbcc808e23a7b16",
            "c534136db5dd4884ad135393d11af79c",
            "d48f573ea9494b83b6f4bc9156941b1e",
            "1e5568b2e7944100b47fcccc8c243906",
            "0371be8531094dd9bb795df8af0cc846"
          ]
        },
        "id": "ZYBoDbXr93vM",
        "outputId": "fd2e2b5e-ed0a-4131-c5c6-328164b98e5f"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(qarib_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "89c4a51b99fa4faabe880db18dc4ddfb",
            "aa495f911ac74c83ba068a2ba748298a",
            "139537e74f2c4ffbb15155578516f9d5",
            "0729061f724b43549f963231e9a6940e",
            "952d77682e4941bcb1905ea66db4c335",
            "97f8ce17d7fd47e7ba7d953f084e286b",
            "b82771e4dcba48a99186be01e4bbb8f3",
            "5c804cfdf1ad416ca9e565ffb8acff81",
            "0ec2842125ce4d898d7a3f9d1747f070",
            "7e1fa60706e74ba6bd9939d7fd863f08",
            "5aa9a7f64def4d13b88a3a4aee5394e7",
            "7a921208cf2b47d3beba908ce7e1e2a0",
            "c19043b7fba249239361a84bcf7413b9",
            "0922b1c0b8b1437fab35221d1798cd0a",
            "94bb52a0dc664b94ad610e67e7c1e3db",
            "0449bd535cad41b18811c91756763603",
            "99af7d4068f443fca3a7bb26f42ec912",
            "37704160005e4721a24f521ff5b036b7",
            "25dddb21c5894cdb9388ef7efaaccea6",
            "aa82381668b34dd88bfcd05e01d6b90b",
            "f59b4b56fc634f85b98eb9d68849d3d0",
            "347b53986c2c49e5843d20fb6d5eb532",
            "7622d08a301a4163b2130e3fde4a7d03",
            "ca39a9000d0544fe948babeb62a900e1",
            "a51929ffc3e04565b75553c8574bfabf",
            "b6044973a87e420580ec48a45419260d",
            "f31b696e5a9a41998c9693512e44b5e8",
            "377e123c5a1f41aca7d869244acf510f",
            "7e35424792ae4fb3a66d53912b741fa4",
            "5b44ff9c109b4cb88501ad3c4e46f3d8",
            "1d7c39144a4f466c93ef8e4b98a25415",
            "63bbf50fd39b4dfbb8d90694cffdc475",
            "743e82f050334fde88264737fbf09aa0"
          ]
        },
        "id": "xzSue81A98aM",
        "outputId": "cb87d35b-8e4b-49d6-9f6d-db4370ead5e0"
      },
      "outputs": [],
      "source": [
        "# PY_DATA_COLLATOR = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "# train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCWrnMTF-AZw",
        "outputId": "359e47f1-0698-42aa-9ded-26801d0c0d32"
      },
      "outputs": [],
      "source": [
        "# The most recent checkpoint file will be listed at the top.\n",
        "!ls -lt {PY_CUSTOM_Qarib_MODEL_DIR}checkpoint-*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "98e496774c29483b8fb3611871573b06",
            "dc1eb34c810443a29cde8ab37d315c8d",
            "b077596f2af442d8b2308c50dd2e96af",
            "25599d9eeb41408fb27e2165305e4c69",
            "2863be4f68fe49c2aa0e3f5de997cb6f",
            "2ce49e79b5f74697b96577a1ca833d41",
            "b4b379aa57f74450b284a1c9cecb1227",
            "14df8de057664362b3f9c20fa5340bfd",
            "74f7d0437b8041bea251303837959f84",
            "25cc03ab153b42ebaa3d53fa9f130058",
            "6e0aa25e4bca43e19f20e8c8edbb965d"
          ]
        },
        "id": "_fuurLfE-vMW",
        "outputId": "cadee986-c6ca-4683-c742-c4e69e17b375"
      },
      "outputs": [],
      "source": [
        "# set_seed(RANDOM_STATE)\n",
        "# model , device = initialize_model(save_dir = PY_CUSTOM_Qarib_MODEL_DIR,model_name = qarib_model_huggingface_checkpoint )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94cPv37I_Asv",
        "outputId": "a3143b10-15b3-4fcb-c664-07fd54675c3e"
      },
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "# summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6e5e7cb6060c4fb7949d30c6d30f4159",
            "f969e94b17c84241bb26c58b7a93cdc0",
            "7daef5f1746e4bd68b8c4090cc9c1a09",
            "283c22209f12431ca96e384427a0951f",
            "09186b2f84ea45a4ad2e130f4c3fff84",
            "db832e3b7ee540a48ecd690f25ec1c3c",
            "931a1def8acc48c184df36aeb99dd880",
            "ab3d28538d7c40f29bcf584ad5a55846",
            "41d97c4e476e4f998b62d284310f958b",
            "c769ded8cfc844d89c2918a25f262201",
            "ce8c268c12054a97a514fe030af78ace"
          ]
        },
        "id": "AH6Zixp1_E1j",
        "outputId": "e50c9e25-1316-4df5-89ca-534589703cb9"
      },
      "outputs": [],
      "source": [
        "# train(model= model,\n",
        "#       train_dataloader = train_dataloader,\n",
        "#       eval_dataloader = eval_dataloader,\n",
        "#       save_dir = PY_CUSTOM_Qarib_MODEL_DIR,\n",
        "#       device = device,\n",
        "#       epochs = EPOCHS,\n",
        "#       checkpoint = None ,#'checkpoint-epoch-7.pt',\n",
        "#       evaluation = True\n",
        "#       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1a43398a44944ba9a9ee99e83222952f",
            "5d457d5132b74131bbce209f03b4f4e2",
            "703d43eff3614d17b3356c56133c2bd0",
            "638daec9425f4e04baa033b57bb1d8c9",
            "7e16283f24b14de8a2f22d5f6e3a8624",
            "972f3fc8b0184dbfb0645ce8fb3ab949",
            "da4c5d8b875e4ee58e23d94a387daa8a",
            "ae0fb3d16556410fb1c9e455c7e22d62",
            "5ab3fa7e81f549eabe6d68deeff8c5ab",
            "cddd460136ff44319bf2e576bfeec434",
            "ebdb3851827341d2b218a21f397b9e7c"
          ]
        },
        "id": "CfknYcbx_Qjk",
        "outputId": "4b416c1a-6fc0-4aa4-a90b-1d52257e971c"
      },
      "outputs": [],
      "source": [
        "# tensor , ytest = bert_predict(model, test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "wQZlHOEo_SZx",
        "outputId": "b6ce3a0c-53e1-49ff-b75f-db68f9d5f805"
      },
      "outputs": [],
      "source": [
        "# print_classification_report(ytest, PY_CUSTOM_Qarib_MODEL_DIR, 'Qarib_classification_report')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "-3aRSjxn_UFr",
        "outputId": "1db0f78b-fe3b-4e61-f664-e8e2b56b7154"
      },
      "outputs": [],
      "source": [
        "# show_confusion_matrix(ytest,title = 'confusion matrix -  Qarib',save_dir=PY_CUSTOM_Qarib_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZhjja1uMayD"
      },
      "source": [
        "###Giga_bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lx_L1HV4FFf9"
      },
      "outputs": [],
      "source": [
        "# Giga_bert_model_huggingface_checkpoint='lanwuwei/GigaBERT-v3-Arabic-and-English'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "04a3aa4fed794a1199f45ee61ee1199e",
            "36298d8003734d709ff1f70d1148b80d",
            "10b791396d4245afa27aab3b5e6ba69f",
            "0790627db6e9454a8f04c87ff5a041e2",
            "5bb624d2e574489d8477e7afa4b97e51",
            "64807fc641e44d0d83cf48a1091c5bed",
            "455578636c674a5293cc254bb9185136",
            "6d3a88e75b3048239b53752e40cb41e8",
            "9d024084e51147c5966b17b3e143e494",
            "0a2c204d61c74456aab32259079373c3",
            "0686dce4c76d4fe7a6e512083e337713",
            "8755541a5a554faf9c7ac6ec08109a80",
            "e0174aa14cac4725bbd42aba5cd5369d",
            "d5acb075b33849999efa323e206f6b3a",
            "2d2dd44e784343858cee574fff0cc10e",
            "e7ca5ba03ba44b338cc5426af01c76e2",
            "ba6215bd775f4788a1c3cabc4fd358fb",
            "5cf31c31cb7941e4a7bb77f7b6c842b5",
            "5c4a5f56da434d048eefd7c4e73ee1a9",
            "42e4587a9ff24d7b921fd8d0d4ad3237",
            "ec510d12b3674e9885367134409ce25d",
            "fb0f4bd2f66e464096f2c88149db13f0",
            "dc5bf639b819460a8c37c777d4c9952a",
            "0783868911d64f3db18e9a52e4df2549",
            "441b9e41d7ed4ccbab92bf9da77f2c7f",
            "5da5dfc8cd25461ea6da06adef38166c",
            "b5ed72f2de8c44a3900dd5885204768b",
            "50a282d9f6294f21b6a02e1bee605d17",
            "40f6b591932b4e6dbc413132b27bdf37",
            "249f51b66b6a432c9394035d6f1f9501",
            "38c57f19812d40b485744c937022cd56",
            "6205ec5d75294e0980d7a89d32e7c7e3",
            "b786aaa0cef346bbbb5a9e00f439b763"
          ]
        },
        "id": "ZCFrfNlSFQY9",
        "outputId": "0998131a-b1cc-4b1a-c264-8436c2a64665"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(Giga_bert_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "af8a3a4d12a849b68b35399d9e739f4e",
            "9caccd41b100475fbdb054bde0ec1d69",
            "724399c91b07425fa67e9809bc7e143b",
            "88634630dbe649f7aa4de38d34623c54",
            "1c48679f1a5c4b22bfa6e2f6e9ac07f8",
            "e8818000870940c5ac0ddc578fb7fd28",
            "5ad6b8ef88fa4edd94e9a0e3e96bfe54",
            "5f61035c7fd846d4bbaab9979a2053da",
            "dcc794f2d8d94cef895d978595f61cd4",
            "8a5ed8f4b705418181d0e11dcbbdce1e",
            "b98e87a48e0a4e0088f22c32b0ba4ca9",
            "7bd31135eba54749b8c7672158b3e294",
            "920227d850ec406388d4b602ff60b450",
            "04149ecae81a496ba2bc1eb85f193739",
            "f6d28e504a904339905cb1628bb16578",
            "f5f063d32c924713b24f960e74f8fc54",
            "bf6dde615c7c45c9a132f8627a4787b8",
            "718dabf5d4ec43c1800de536bb8ae2fe",
            "a29793f99ddc4c01a3b904e3a027ffe7",
            "e87a6f815e8b4b558532ad515562e5ab",
            "1bae9157e86942ada7c4184a0efcebe8",
            "d974103cedbe4adcac27a9e8bcd37549",
            "eb5f9ab1e5724bda922f4d8739d27b13",
            "1698fbcc2131417c9a340b090b6e70c0",
            "50c5fdcea37b4e4b801278c7f631a533",
            "f782a35aad8c46559d26103dd35178aa",
            "43371bc95d90459db4d72b61d210f0f8",
            "563a623559f54b86a2ed38f1f9f23d65",
            "524e92ae038c4438a16addfabf84d675",
            "d30c8b950f224bebadefec739fc649ca",
            "c97b3bd000734f10bfb739c8f6cbfec0",
            "5d938e3841da4c36ab1cfc55713eb17c",
            "2dd1ec5b82324b1aad76e240c3583e1d"
          ]
        },
        "id": "i-SY_SuNFS2Z",
        "outputId": "cff9ee34-a36f-4f7c-aa43-85d0d25613eb"
      },
      "outputs": [],
      "source": [
        "# PY_DATA_COLLATOR = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "# train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0tS49R4FVA2",
        "outputId": "74494278-ee51-41f3-f38c-a803d4a21a17"
      },
      "outputs": [],
      "source": [
        "# # The most recent checkpoint file will be listed at the top.\n",
        "# !ls -lt {PY_CUSTOM_Giga_bert_MODEL_DIR}checkpoint-*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mU38LhuFuKp",
        "outputId": "2dad52ee-752e-4964-960f-5671c0acb1da"
      },
      "outputs": [],
      "source": [
        "# set_seed(RANDOM_STATE)\n",
        "# model , device = initialize_model(save_dir = PY_CUSTOM_Giga_bert_MODEL_DIR,model_name = Giga_bert_model_huggingface_checkpoint )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cfbf65716899403d864baced1bacba6d",
            "5ac12ea708924941a6b76df0d9e306f4",
            "fd4d127e47dc4c46a2126c3ba20d505b",
            "7701e13730434e17b6221740c27a419d",
            "a9d30506dd7e46c4b9fd701fcb566965",
            "f559e480e4674cf8a8f02624bcf8b2f7",
            "7b4f43ecabf947ee8ab635e5d4f7573c",
            "b4d6c8ce2b5249d3bf9b7db76b2cb85e",
            "6d070bce6b6649188710691719edecb0",
            "397dfedc07a549429caeb90bf80ab01d",
            "346825a17d0a45e5bf942a00b2467834"
          ]
        },
        "id": "CwsTGpATF4Q0",
        "outputId": "4fbe1db7-c25a-44cf-cf10-b416156d0791"
      },
      "outputs": [],
      "source": [
        "# train(model= model,\n",
        "#       train_dataloader = train_dataloader,\n",
        "#       eval_dataloader = eval_dataloader,\n",
        "#       save_dir = PY_CUSTOM_Arab_bert2_MODEL_DIR,\n",
        "#       device = device,\n",
        "#       epochs = EPOCHS,\n",
        "#       checkpoint = None ,#'checkpoint-epoch-7.pt',\n",
        "#       evaluation = True\n",
        "#       )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "57b10e555a20441bbe152fb12fe8f860",
            "2aa776938e3e4091b8e77ef7bcd29fcb",
            "1e003672576d46389cca7df68526ae3f",
            "d99757ba9daa4347a01d82e4dfd56205",
            "20abcbf38a9d429eadc584c5d03f969f",
            "98bf637a5f9e490687475db1874cc2fc",
            "de7c12b723fe40a58f67e03147a9959d",
            "78f579f868c84ddfbacd355ffc984b9a",
            "62b6ce6a1f4b40ca803028e0f7697d1b",
            "8a0c1095ee8f4bd6975e921f0992ab73",
            "fcf4503caeae45c09e2f1ae74cbe1720"
          ]
        },
        "id": "ni-dyE4CF7V1",
        "outputId": "8e19ea00-db33-4050-e1dc-973829818090"
      },
      "outputs": [],
      "source": [
        "# tensor , ytest = bert_predict(model, test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "FB3V5xCaF81T",
        "outputId": "056df0c4-1a70-440b-9d09-35631710fc0f"
      },
      "outputs": [],
      "source": [
        "# print_classification_report(ytest, PY_CUSTOM_Giga_bert_MODEL_DIR, 'Qarib_classification_report')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1OGoWmrF2xR",
        "outputId": "b1b7215b-3b50-4e96-8f57-ad51a9442039"
      },
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "# summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zAuDdxGfRLb"
      },
      "source": [
        "### ALANZI/imamu_arabic_sentimentAnalysis\n",
        "\n",
        "> تم بناء هذا النموذج باستخدام مجموعة بيانات عربيه مصنفه الى ثلاث تصنيفات ( ايجابي ، محايد ، سلبي ) حيث يحتوي كل تصنيف على 30646 نص .\n",
        "\n",
        "> - دقة النموذج ٨٤%\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnmCgGLlfRL1"
      },
      "outputs": [],
      "source": [
        "ALANZI_model_huggingface_checkpoint='ALANZI/imamu_arabic_sentimentAnalysis'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We-OAGwHfRL1"
      },
      "source": [
        "#### ALANZI/imamu_arabic_sentimentAnalysis Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "8b2a046d6bd644e4a540052ca29876f4",
            "a8b3de7e93064ed09e669445cf79f48e",
            "a041d75ca7c54cfbae8e88954953da86",
            "8423921b90f64db3aad442c53e241c3f",
            "40211259c3d64b368875e753055ebfed",
            "3465bfa68fe4484db4eb253d17d8cf7b",
            "986477cea4924343a09f6c474c85385f",
            "1c09efa474ca45b4be4c0b508d30ed69",
            "b014098a53a54d4a832b96b31193ce9a",
            "204fdd354183406a8d38dc7c21fdb678",
            "63f0ad1fa0014d78b004b41d2f93b760",
            "4c40512185f545688a15b471781ec1aa",
            "1d90200ddcd84a1185e9f92d77ff4ccf",
            "74d83e80256f475d9aadfdec2a50e25c",
            "f5475aa18f324a73a7dc2f762824a0aa",
            "89e6537a515743deb6683f99ec042e52",
            "e67fd944dc254b70b02907fb0b155bfd",
            "15cd7d016530459c91a6b6a2370e765a",
            "224c85fc194d4282bd3b7d441fcd25c5",
            "f90c83c3a4ae4b33b21f4d2e0ede3f12",
            "2bc07e5f4c9846288f5a12b35afcc551",
            "6304900b86c840129c23cc3f61dcb38c",
            "910ec823996848fc9e82ce3648bfb71b",
            "314ca447c37b43ba84b242af8900bd2f",
            "e2d13c9766ad404ba82772c01678e789",
            "fbb73960c2974f84b7c1f61ff446410a",
            "d62c9439be6d43f98ac6853082c60d1f",
            "3d2d8169e92e46e8af76f0cde3e2d41e",
            "0e35ec759b42421cba181236b142cca0",
            "3799c8ea8122474a820c222234246b1b",
            "fbb887a7bfd945d9878e4abe5c73a76f",
            "02ac06683f234906b67495d91c0ffff6",
            "4bf3abfacf234694a71f1c656694630b",
            "6cef219f79184541af140b8dd99672eb",
            "585dbe094f8240cfbf708ff410091bdc",
            "de7b4f2a5a3e40949172e84124e4148a",
            "77cdd7ba0ace45eeba74e72a9db3d7fd",
            "e31684def3d244b39d2a7bd36412fe45",
            "1d4ba7e73bda418d9d9a9e4cac05a86f",
            "1b943fcc3bce41dc9b470f07b731da2a",
            "d764fe3667984cc589f5a507c2d862e7",
            "304eb52a90dc447f809f7a7010cb5271",
            "6ef7b397b4f243b59dfb06b4561c014b",
            "108ac3aacdc64895a4166c4397118f9d"
          ]
        },
        "id": "DwuOPatFfRL2",
        "outputId": "e5754291-932b-4209-aba5-c0df6eb3ae93"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(ALANZI_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "dcb0578c94854136998dcb41d19c820a",
            "a18d94eaf6fb46a286ece9b0046d5567",
            "caabc0953f30440089cbf3a09f58f0f4",
            "02af9f7b23bd4870ad0c6dd538327915",
            "f175c83f08d34cc187c325898d70d8da",
            "8a9c97a0b65e450ea7c72faff29795c6",
            "8fd1e5135ed24b738962ad3c86e83dc1",
            "4332466bfd544d2ebd6ff96dc4a3ddc1",
            "4705e2ffa91243a0810aea79e6b5db4a",
            "244af762469a4e02b64aac1e638690c2",
            "15faedbe24bd4d618264022e99ed3828",
            "f0f60a319533423ea9d86ca5dc8e6e66",
            "b53e0688a4ea4ec3b66174784480469c",
            "4498c72515384910a1951c31c45b50cf",
            "1a46b1aada7c45b0b45a868516709e67",
            "a25446fa521547ecb7cb57960badea4c",
            "c4e63dcc01de4b93bc6919ee9ad54b4a",
            "c257a2f3c81f4a5fb08e91970f4ebb36",
            "357b795663b34453a23242c060469f2c",
            "b0108dddeaac4f13ba284e88dd6ea621",
            "b27d900b29be40a98779710c59397119",
            "eb9e79c9c09f484a8bc43fef24d85a87",
            "09bd1ce551ba46e29daef8e6a8846058",
            "5eb1dc0dae864eba8f670dbd64648895",
            "80a75783554d43aba5ab95b4b30ecce3",
            "09d7f5b384ba445eaba750b7b5fe1029",
            "63d7228efb0c407aa90debfcf157f4ef",
            "c95371f0943941e581e55d57c055d554",
            "2cd0bdceb4004557bb05f43f70861121",
            "fb5e319c06cd435f97704b5acf0c9cbf",
            "6aa6cdcda4954cf98d0c147b9534b48f",
            "1e2f4ef9ea1f44be960dec3b3ac6dcaa",
            "bbab26235c134fcebf89d2ebc3982609"
          ]
        },
        "id": "5U-NDbHlfRL2",
        "outputId": "ec71968c-09dc-47d3-87d5-1bdc9ac17696"
      },
      "outputs": [],
      "source": [
        "PY_DATA_COLLATOR = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5QFlzHifRL2"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqP3YkwEfRL2",
        "outputId": "23faef59-6d12-43df-e72d-e526da33bf1b"
      },
      "outputs": [],
      "source": [
        "# The most recent checkpoint file will be listed at the top.\n",
        "!ls -lt {PY_CUSTOM_ALANZI3_MODEL_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445,
          "referenced_widgets": [
            "650f5ecc1be74e1f8affbc9b01ffb247",
            "e9c9633cdc724bf2858a8c7b304a7177",
            "9246473929524756a5aff00c474a2c5e",
            "6162c58f7797460eabe41dc42458c917",
            "3672a95716c8442791f17ebea6151598",
            "0305adb049fc45fe972451c907816b18",
            "18ab7016570941bab3608f3439b38690",
            "ae6bfc744e2543e8bb35364bb241da67",
            "c48d28e5acf74399b8a424f8ee360c57",
            "fd2a648ccebb42fcb3fd4260cbc39f85",
            "2ab0dbb2781d44dd9486f5ed766f916f",
            "e1d7cb8f72d4424c8ae5aade397e3659",
            "3d01d11d1ba14b4eaad27e937d88e4c8",
            "21bb6b4fde3748f7bda6e6c21cc809f4",
            "aebf18c11ee142d2bb84c1988e93527d",
            "a81aa56646f04f4caf09861988c3c4eb",
            "13f4a00368a141b6b5b0db2ebda0c152",
            "55cdda9e2afe4fedb3e25c15b232c87a",
            "6edc82715dcb4abb89f9e4e14ae0c2c2",
            "052c94c6d6ab40b1b569767fbaf3ad4d",
            "2ebc65ead0bc484fa052f458916c1228",
            "818c98053be540b5993d3d993f7af839"
          ]
        },
        "id": "Jtotj1e2fRL3",
        "outputId": "22c63deb-0069-47ab-c3b1-c00b34b19337"
      },
      "outputs": [],
      "source": [
        "set_seed(RANDOM_STATE)\n",
        "model , device = initialize_model(save_dir = PY_CUSTOM_ALANZI3_MODEL_DIR,model_name = ALANZI_model_huggingface_checkpoint )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwDfrhGEfRL3",
        "outputId": "0c4a2222-59c9-478c-bd38-0bee00b0f4f4"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3ea9be1fa87e458883dbac1351289401",
            "67842bdcdc9e4434b0188c8a79f4aa83",
            "aac2df7fdb3e473cba9fb5362dc75f0f",
            "62b8a66c47854fc8aaf1ce6eacd2a3fc",
            "2cc0a46c119f41e7a6541c453ebff895",
            "83ea6066450d418ab7b1b7e48eb4c1dc",
            "8608c5dd71c742e487eebaf26df3aad8",
            "e07eaa32d61147d59e1d9ba451dad6a7",
            "5652ae9d52c3463fa8740544c30d0f41",
            "5fe3bd1a715d4f6d93d44fff00399896",
            "8b6519dc7e4e46fdbd3949c69de5ff65",
            "7d9bbf0640d046edae98d771a1d53e67",
            "ea6338d14eb741ce87493b3202baee5d",
            "4d274124490a45d1b8c9319b2d7f9f73",
            "a42211faed7147ffb7dc82cdeb7c809f",
            "eb79a2a0f2654f69ad810a349021e6ae",
            "3fa54cec0ca2422398488f409fea19c7",
            "927587d39be34bb19b875ea22b181f61",
            "13e917de54ff4a83abc9bef3c563ce22",
            "ade23057c742433a8c827695b69fa177",
            "262c4d0f9cfd4954abd90abd99871a93",
            "ed927abba2274ea5b69240411cdc61c7"
          ]
        },
        "id": "YwDUI0tSfRL3",
        "outputId": "f7b9ae9f-80fa-427f-e30c-ae34ca9d8695"
      },
      "outputs": [],
      "source": [
        "train(model= model,\n",
        "      train_dataloader = train_dataloader,\n",
        "      eval_dataloader = eval_dataloader,\n",
        "      save_dir = PY_CUSTOM_ALANZI3_MODEL_DIR,\n",
        "      device = device,\n",
        "      epochs = EPOCHS,\n",
        "      checkpoint = None, #'checkpoint-epoch-2.pt',\n",
        "      evaluation = True\n",
        "      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SB9zqdVQyof"
      },
      "source": [
        "#### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6a0f1ee6942241e6a299f34e9ae7aae6",
            "d34461bcf56a4ba4be79f7ecd22967bd",
            "9d1505f998c64fecad30497d683ee396",
            "621e9bb529f64033a5ef4325bb7ebedd",
            "3c0dbb204d204e8193114f46f26159a9",
            "49a9471cca494ecb99afb0accc8b737e",
            "392e8267fb9440e88fb7bcffa167ef4c",
            "ea757f43713642dcb5a435f87e7479d2",
            "567b5fb8f2c14825bc1fca316548ae69",
            "1622a9a5b57542ba807283b1e021ef4f",
            "52a3b63519084626b7d62cc2b32a24cd"
          ]
        },
        "id": "SyLKQGQkQyof",
        "outputId": "d298cdea-fdca-4fc4-d526-624f80df5383"
      },
      "outputs": [],
      "source": [
        "tensor , ytest = bert_predict(model, test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "EB6oukJfQyog",
        "outputId": "7c1d1918-e8de-4181-9795-9c895dd67e8c"
      },
      "outputs": [],
      "source": [
        "# Assuming ytest is the output of bert_predict\n",
        "# and you have a directory PY_CUSTOM_ALANZI3_MODEL_DIR to save the report.\n",
        "\n",
        "print_classification_report(ytest, PY_CUSTOM_ALANZI3_MODEL_DIR, 'alanzi3_classification_report')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "nMpOTywTQyog",
        "outputId": "d3dbc66d-48c0-4c13-df65-7b2be658d2f2"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(ytest,title = 'confusion matrix - Alanzi',save_dir=PY_CUSTOM_ALANZI3_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhvyUQlLjqKD"
      },
      "source": [
        "### ARA-BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQOwVckLjqKE"
      },
      "outputs": [],
      "source": [
        "# ARABERT_model_huggingface_checkpoint='aubmindlab/bert-base-arabert'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2FiqZoJjqKF"
      },
      "source": [
        "#### ARA-BERT Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317,
          "referenced_widgets": [
            "957fad73f4b142bf980fe02e48b97644",
            "79478d2d7f184aae958cb95860238d53",
            "70862ad5bc3d4aa59f662e9045275746",
            "71d158973fc94a5fadad45b175ff1275",
            "107389afb8734bb9a2fd5d957b69b087"
          ]
        },
        "id": "HtYV0OjzjqKF",
        "outputId": "ae7b9304-47d1-4873-e525-f6284d82e5da"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(ARABERT_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "93be3a4744094ab2bfc8b7d0f1b372f3",
            "228b4011f98e47298d802803b4bf71a0",
            "8207e999b7754308b84e528db5bbc5a5"
          ]
        },
        "id": "4WXyt0vKjqKF",
        "outputId": "05e0c336-bc72-4100-821c-d98c3753a9b6"
      },
      "outputs": [],
      "source": [
        "# train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmL1ykE9jqKG"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MyEUXY6jqKH",
        "outputId": "cec2c0b3-05a0-4463-cbba-975d525abfb2"
      },
      "outputs": [],
      "source": [
        "# # The most recent checkpoint file will be listed at the top.\n",
        "# !ls -lt {PY_CUSTOM_ARABERT_MODEL_DIR}checkpoint-*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514,
          "referenced_widgets": [
            "31c37f6a84274c0396374755c0345515"
          ]
        },
        "id": "mjaEiHpNjqKH",
        "outputId": "a23a1f83-59c1-4110-852f-cdc9fc8c1023"
      },
      "outputs": [],
      "source": [
        "# set_seed(RANDOM_STATE)\n",
        "# model , device = initialize_model(save_dir = PY_CUSTOM_ARABERT_MODEL_DIR,model_name = ARABERT_model_huggingface_checkpoint )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n-_uaRbjqKH",
        "outputId": "ea2a1919-1408-4af1-e14c-13f6e514b273"
      },
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "# summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "22a1f5edd0bd493987f191f789648da1"
          ]
        },
        "id": "EU8LkSnBjqKI",
        "outputId": "83d1361e-05cc-4ba0-b6c9-42102fbdeffb"
      },
      "outputs": [],
      "source": [
        "# train(model= model,\n",
        "#       train_dataloader = train_dataloader,\n",
        "#       eval_dataloader = eval_dataloader,\n",
        "#       save_dir = PY_CUSTOM_ARABERT_MODEL_DIR,\n",
        "#       device = device,\n",
        "#       epochs = EPOCHS,\n",
        "#       checkpoint = None,\n",
        "#       evaluation = True\n",
        "#       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvGvBo6xFXpf"
      },
      "source": [
        "### We considered the one without the classfication layer because it is much better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689,
          "referenced_widgets": [
            "2f3030f192284a0ea6f11e0d041a4326",
            "be839fa25e554403a8c876399e8afe6a",
            "3f19acd78ac0433d8024b2094533e637",
            "c230e7709c1a4a8f886080667718442c",
            "fefe82673afb4ffc87196d65f18a1f58",
            "2c582206d77b4eca82747116b9d92f97",
            "a8422ac7b4d34bc3b8c6fec1cdb52663",
            "74d4b81b5dac44b996f04b304180e08c",
            "62a27f890e294f2bb91f261de75947bb",
            "38b6545e493d4a28b183c7a924a78573",
            "fad6139cf419474dbe169f215afa9d3a",
            "0a91aa1b91e9487ba645f49f02243f57",
            "527f9803392248c49c946f40cc79161b",
            "dbfd9c8ababb4a638647aa2fd1dbee8e",
            "a95b13a2f6f94c9899614cc12cfea43f",
            "1691e31950564e83affe6af325090313",
            "7808d43f54b442879a5149c71f9decd3",
            "f6c0c0c5d1b84e818c9b1b9460c1da10",
            "e6c03a9822694841a890c468bf15e13d",
            "1e1a7b52775e48a29a6bccb63bf4d184",
            "c0849afdbdc3475ba1fd70a215dfb732",
            "37b21ae26a194c2ca0f0a62c37635bea",
            "fb21ab7adf024eec8b23f1a6b3ff3a32",
            "cfc3e93a25524db49b95c07224e17fdc",
            "673767541d1b4829a435f8444b414928",
            "fa5bf5811ccb4ec89fd577f193807970",
            "2b9f8e0aea774222b9f3955a8e0c3c8f",
            "6446e34d64414413ab403ec14734850c",
            "f0be95aba66e4821848cdaec53615027",
            "56d86d8fd1bb4e789549d8faa2bd17b5",
            "dc9427c9146d4a7cae81a411993c27db",
            "f42acde8846143e18e65bf8f38fca0d2",
            "6e9ba8f475674ac395b221956254ff68",
            "e23453f1d8de40028a481eb12a52772b",
            "d2d537248a224bae9f96caee2f9d0c4c",
            "cfea942f940f46e4a37567876cb22355",
            "5c581e7e992044eb891da66910be4375",
            "35e2539e17a44fd7a660cf2bd31c9d84",
            "b965a57e09a54b0d8e2088e725f25448",
            "15c34935ffbb4b9cab6ed825d865f30c",
            "72b7103b4db3444e876b69ffbfe0a809",
            "071b656d403d4de1b204e8557774d999",
            "1c9a9f74672449d387b5bdbe0cc3bc04",
            "e2f5739bbd794b5bb77fff3a472e8d3a",
            "de825e2635364df8a1bd82a0def573bc",
            "38307ccf800643e0ae348ee06e64697c",
            "46b16b1a547b4e608331b18fb2259d61",
            "776ff434c2a14f43a879e99066401a49",
            "fdcce8746fd1428a9e3cb0477660fcbc",
            "087350141c794e02a5e3572026d50d7f",
            "5b276d9d2ada49719de5ddd4b8fd7092",
            "ef37dc9edcf24e6bb61e9f97024f4c27",
            "f3c0bb4ad3b84f3fbc143f7b6b4d7a93",
            "2210646963644fe1b12a77da957c2955",
            "3b8bfb30e78f4962bd6fc3af85e15ae5"
          ]
        },
        "collapsed": true,
        "id": "Cmg9pt3SExnu",
        "outputId": "f0f10734-5c5c-4c22-9300-ff877c07aec2"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# # Instantiate tokenizer and model\n",
        "# tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabert')\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(PY_ARABERT_MODEL_DIR )\n",
        "\n",
        "# save_architecture_design(save_dir = PY_CUSTOM_ARABERT_MODEL_DIR,model_name='AraBERT',model=model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3nblZuHFeup",
        "outputId": "dd6c72cc-c10e-4c2c-9fc1-f8f496910fa6"
      },
      "outputs": [],
      "source": [
        "# # Use the loaded model for inference\n",
        "# inputs = tokenizer(\"مش بحبك\", return_tensors=\"pt\")\n",
        "# outputs = model(**inputs)\n",
        "# logits = outputs.logits\n",
        "# logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwWz7xRTnkg1"
      },
      "source": [
        "### MAR-BERTV2 (arabert and marbert)\n",
        "\n",
        "> MARBERT is a large-scale pre-trained masked language model focused on both Dialectal Arabic (DA) and MSA. Arabic has multiple varieties. To train MARBERT, we randomly sample 1B Arabic tweets from a large in-house dataset of about 6B tweets. We only include tweets with at least 3 Arabic words, based on character string matching, regardless whether the tweet has non-Arabic string or not. That is, we do not remove non-Arabic so long as the tweet meets the 3 Arabic word criterion. The dataset makes up 128GB of text (15.6B tokens\n",
        "\n",
        "> Arabic-MARBERT-Sentiment Model is a Sentiment analysis model that was built by fine-tuning the MARBERT model. For the fine-tuning, I used KAUST dataset, which includes 3 labels(positive,negative,and neutral).\n",
        "\n",
        "> [hugging face](https://huggingface.co/Ammar-alhaj-aliarabic-MARBERT-sentiment)\n",
        "> [ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic papers](https://aclanthology.org/2021.acl-long.551.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOvpHZI-nkg2"
      },
      "outputs": [],
      "source": [
        "MARBERT_model_huggingface_checkpoint='UBC-NLP/MARBERTv2'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tslMndVhnkg3"
      },
      "source": [
        "#### MAR-BERT Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "3f8587890c8b4f7f92de3e06bf727973",
            "24a0270a32274742b7ab501daaa12df2",
            "58098d85ed884f56ba220e8161385b83",
            "d5b741b853f344d092e34edc3b1e984d",
            "f7597d8a6b344e20ac3d4ec579ea3327",
            "fcd2c3d1e86a447b862bfdbc2d643fb6",
            "8671217c7d3e46b1a1c0164a47cebb14",
            "b27ecd22657047118c9aea2b1b6994db",
            "2be06e8b008c4a05b79277d03fd2039d",
            "a7d879fb53324e2da3f2241f4e9a7160",
            "02f2ff76a9e24ed28f8e28ad7849bb17",
            "6bf2f44006f5406687080f39d76d1bfd",
            "d8f4f963a16b4f28af03866af2350edc",
            "b60dc38892b64409a99cc91d272fae9e",
            "54e43dadb96443c1969aab8ca62e9278",
            "5c9aa2af5eac40cc821ed3e80fb27a49",
            "577d9d9585014fb3821748601c3f4a20",
            "f7c58a5318344db8bf81b245e8221060",
            "7b7714dfe9d948d996292a5c70c4a213",
            "5de22665659746fbb1608d518c401e33",
            "b4df91c74fae4490ad6fa185ab309769",
            "593a50baf3d644fb92b68f3e3fb68af1",
            "3a3601437e0d47e7b25ac360a4ae1234",
            "574b4b62abc445cc98c2d9552c99f8cc",
            "33fe2a8e8a0b484885617015d019a0cf",
            "d10f179639204309a85009b5c613dad6",
            "127c7dbbd78b4b389dd9d51ee049129f",
            "062457778144429691084cb6d4b343d1",
            "37ea88e90ed14036b3d4b9b8c0e31e2b",
            "8f4871eac3ef47fe91c79973509da355",
            "4675547014004f9ca7f8cb9020b2022f",
            "23c0034294954a0794e4d01b488e7155",
            "24bd757d3c474f7790f3493c7d4cabbc"
          ]
        },
        "id": "827Hi1J2nkg3",
        "outputId": "4b0847af-b94d-457f-aacc-3ef9be234829"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MARBERT_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "7953c5f98904486eb63bcde2c6db16c3",
            "40c56f54b3e3483da411368b6b065322",
            "28c12e1a33e44114bda0c2ecbede1831",
            "cdc7e4e963794c1092bba8b107f84e43",
            "8be33ca620c841cfb1aa826155b83a59",
            "de46f750f95c4c5f9531dfc6c8e7f3d8",
            "c215d59369fe49ffb5bfe84d54fc5c72",
            "a4f2513fde2f4e85bd591af7076db0b4",
            "bddd39a7c0434e958a7b1aa2fd520851",
            "07741aefe671420aadba3f37d9d5f0e9",
            "a9d4d031ee5b4f77b2833ba9ad164e9d",
            "100de984b34e497ea5d53a44223dfd42",
            "6ecb46ed25ab4752817f09f40aeb8395",
            "d0249f24300543eb81b4b98b9eacd8d4",
            "60cbbe522d3a4fde93fc500e5c19acd2",
            "eaed33d81db742da8248a55b8d7656a3",
            "4eb3af5310f444e6bc45cfc262049382",
            "904334dbd6114096ab8e2ff566d29427",
            "46da95e824f74cf0bb73c9ddfeae1342",
            "e235d06e124d4973a8a6e38aba114188",
            "03ab8cd2bd4143b882a970b2b5cb500c",
            "8ca310f98e714eb29e7e675987089570",
            "1d4fcfea958f4f57bde55d93301769e0",
            "319aa8586d7f4000900bc7b540083132",
            "b80ab82aec2b40179286f6f5c1d305da",
            "bf893dc5253847aeb505a4cc43f08268",
            "12f6d6fcf31849e9bf2a487019ea3193",
            "a3bdb3317180468dab9b70472fc675ad",
            "48456295f39f49eabd0dffc072733215",
            "a542a6c0996e4ae9840b5d8657d090f8",
            "efab049a695740e8973e71f578931df5",
            "9295a911c6f04da6b5a7d1d8a6f15859",
            "dc89c89ce47d45f39af0a06472a70828"
          ]
        },
        "id": "U5wkTrdBnkg4",
        "outputId": "783a376d-e0d1-4772-8fca-cf8ba4439969"
      },
      "outputs": [],
      "source": [
        "PY_DATA_COLLATOR = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toBU9w8hnkg5"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntKJ2uLSnkg5",
        "outputId": "5ae442c1-1c6a-4423-bbad-4bd8cfe7a77f"
      },
      "outputs": [],
      "source": [
        "# The most recent checkpoint file will be listed at the top.\n",
        "!ls -lt {PY_CUSTOM_MARBERT_MODEL_DIR}checkpoint-*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "20c86675cdd84274bbab2ff966285630",
            "a3e369c09c4c45b488a600a952206ad5",
            "ab02ac9dbdf5421d94acfb1c4f3b7489",
            "e3e3e3ba80ba4131b7db28b5fafde2a0",
            "dc2af5b2f61f4580b8500275d0494f93",
            "d470d4bee992427984a2b0f9c6d4be73",
            "e5f7210e12cd48c7aa42b9e351ec2ef9",
            "f0f1dd52a1704d10bc3577761ac19f57",
            "d10ad5aebe3f4483b0db81f021cd353a",
            "a08bcf339fe2462896034d485116f257",
            "2a2b572538324729a434362797bb94ad",
            "1595fac7cddc4553a4f04afcc6019261",
            "b07845d8fcc54a30bd30d7a2f73931b3",
            "35694284fa3d4e6087673333cef1c8d3",
            "87275e8d2aa347af866ab16f3daf58ac",
            "0dc8067ccf37404693a6d503e24cc44a",
            "a7e894e013464b0d9fff81e367d2da6e",
            "85ab0b42e43640f19443dd71a3b33750",
            "7e3049d6725c4d179d553257f2e14708",
            "33a6d6d5ca144e93a7912c4c5b39bcf8",
            "541ead6bdff244a3bca7b736ea7b9ced",
            "fcfc612136b04c1fac164381c5ad3ec9",
            "0f537834fab246d795c7ae8d6f36d7f8",
            "a4a24f55a75c4b24899a33058c032b8c",
            "65be0abbc0384f8695257a9c4456463e",
            "6af5e9b3109149f49ae0709433083256",
            "b3d3e170aa36491797f400a09003477c",
            "71bb134ec2644c92ad1b9c879aa38b5d",
            "04b0afe6430246b892ec694aaca03f03",
            "b48ad186257441a79d8f953128d7e20d",
            "4cbff0a2c583453aa8ef81367fd29906",
            "992af816d43d466c9f13b0c23ffb9c64",
            "5f2fc75f37754a63a70b44f8a87af0d1"
          ]
        },
        "id": "PHH5F1X4nkg6",
        "outputId": "d0494996-f0e5-46a3-da29-bbc693debba8"
      },
      "outputs": [],
      "source": [
        "set_seed(RANDOM_STATE)\n",
        "model , device = initialize_model(model_name = MARBERT_model_huggingface_checkpoint ,save_dir=PY_CUSTOM_MARBERT_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5aaa013ed44b44819a5a963633b2cded",
            "93cc8b94846f44cd9164da1680de275d",
            "a8d3a15d8882432f9c88b3d8d3fec3d5",
            "d223706d43c742b29bd9f6c90f0e43ec",
            "f57cbe6e1c344471a2c5757dba868c0a",
            "bec688eecc3842f3b5018f2f6ca112fa",
            "73db00e32e5b4a2fa705d5a839bd6f92",
            "e25b3fe8215c40f8bab9178a42129dbe",
            "fba5b9c52ddb4548a1c9b4468af10339",
            "38ef1b86fb934582809977a5530cbf06",
            "10f120b42a6f47609c20bebffb390280"
          ]
        },
        "id": "ILP38U5fnkg7",
        "outputId": "0233a686-1f50-4db0-a8d3-ef7187af0773"
      },
      "outputs": [],
      "source": [
        "train(model= model,\n",
        "      train_dataloader = train_dataloader,\n",
        "      eval_dataloader = eval_dataloader,\n",
        "      save_dir = PY_CUSTOM_MARBERT_MODEL_DIR,\n",
        "      device = device,\n",
        "      epochs = EPOCHS,\n",
        "      checkpoint = None,\n",
        "      evaluation = True\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c766033674e5496a8f6d4f10f79f90f5",
            "26784e786b9d475f8b62764f178274c7",
            "4e46a40959f14ae7b03004b5acf164b4",
            "819fb6cfd6514c729d86d5f662c1ddb6",
            "acdb1333764a4cb88f781a6d020ef177",
            "4d2cda4d73b949a099079b61eea0b715",
            "acf17242b4a243df8b115055caaf7e92",
            "aa84b78564ac4f4790db74531b1ff834",
            "d8c346577eef44aebc1434989c0dfee5",
            "5f94eb17df014eebb1ab0fe9433752c3",
            "fd83c49e19cd4460b7cdb4c52f921883"
          ]
        },
        "id": "GHENseRF8FOW",
        "outputId": "7fb3904e-f844-4b24-b228-ae40c1e1eef6"
      },
      "outputs": [],
      "source": [
        "tensor , ytest = bert_predict(model, test_dataloader,device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "yxLpMNkn8PI4",
        "outputId": "f767e68c-52ae-46d9-b327-c64eb0b4233e"
      },
      "outputs": [],
      "source": [
        "print_classification_report(ytest, PY_CUSTOM_MARBERT_MODEL_DIR, 'Marbert_classification_report')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "d0-k2tob8qtZ",
        "outputId": "bbfd8326-e53a-4d91-c80b-e39f7f35d5a8"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(ytest,title = 'confusion matrix - MarBERT',save_dir=PY_CUSTOM_MARBERT_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ggb-VRu0nkg9"
      },
      "outputs": [],
      "source": [
        "# # plot the training and validation loss/metrics over epochs\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# train_history = model.state.log_history\n",
        "\n",
        "# # `train_history` is a list of dictionaries containing the training history\n",
        "# # including the loss and other metrics\n",
        "# train_loss = [x['loss'] for x in train_history if 'loss' in x and 'eval_loss' in x]\n",
        "# eval_accuracy = [x['eval_accuracy'] for x in train_history if 'eval_accuracy' in x]\n",
        "# eval_loss = [x['eval_loss'] for x in train_history if 'eval_loss' in x]\n",
        "\n",
        "\n",
        "\n",
        "# # Plot the loss curves\n",
        "# plt.plot(eval_loss, label='Validation loss')\n",
        "\n",
        "# plt.plot(eval_accuracy, label='Validation accuracy')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVRpEBSnkg9"
      },
      "outputs": [],
      "source": [
        "# evaluate the model on the test dataset\n",
        "# if trainer.state.epochs:\n",
        "#   result = trainer.evaluate(tokenized_datasets['test'])\n",
        "#   print(f'evaulatuin loss : {result[\"eval_loss\"]}  , evaluation accuracy:{result[\"eval_accuracy\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDpgh3otJzDx"
      },
      "source": [
        "# Ensemble Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI0KkuUANiF5"
      },
      "outputs": [],
      "source": [
        "CAMEL_DA_model_huggingface_checkpoint='CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment'\n",
        "#ARABERT_model_huggingface_checkpoint='Yousefmd/arabert-sentiment-analysis'\n",
        "MARBERT_model_huggingface_checkpoint='UBC-NLP/MARBERTv2'\n",
        "CAMEL_MIX_model_huggingface_checkpoint='CAMeL-Lab/bert-base-arabic-camelbert-mix-sentiment'\n",
        "ALANZI_model_huggingface_checkpoint='ALANZI/imamu_arabic_sentimentAnalysis'\n",
        "#qarib_model_huggingface_checkpoint = 'ahmedabdelali/bert-base-qarib'\n",
        "#Giga_bert_model_huggingface_checkpoint ='lanwuwei/GigaBERT-v3-Arabic-and-English'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4mX-GkTVST"
      },
      "source": [
        "#### CAMEL-BERT-MIX Subword Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGDaHZsNTVSU"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CAMEL_MIX_model_huggingface_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "a07a5819fa1f46969c4ddb336c28e722",
            "5845d23e78ca4dd3935881eee709c417",
            "07c2a3b660ef4205bf1cd27e897a7869",
            "d045e14cd2cc4a11ac48b349fe1d6174",
            "869a33c23a964a7b824c24c84659e45b",
            "ffd57aa71b7244d7b47f853ce4bfa308",
            "627d8ce294954a4abc5a63e8165e0531",
            "16c37e07509b4ad491e244727f51f6e9",
            "beee06e407644953a9314b0cb3e102ee",
            "125e292cdcc74abe9889b6e84db43eea",
            "8776552d429042cf81d432fe0c7e5a20",
            "589edee4a66941e5a76d966eba33936d",
            "394e45b6a64841f08f504146611c8509",
            "9cdd75355ebf41d48717177ecda090a0",
            "2c9abf2126c349559c745f9ce1ea093a",
            "473faad698954b0d95a1eacb80121509",
            "4c26261f14b7464b867b8a6289a291b2",
            "0195cc7c0a1f4d46a177228bc456a9f7",
            "6bfbb575b1264b5e93510b11d6836a83",
            "374d50ba830b4220b8589dccc294f44e",
            "69d58d06756b48e0a227514281b96687",
            "7ad3aa8ca52f45f293d0c87534967fed",
            "6ac953d3335e407abfd808e2a262538f",
            "95f82ce3d75a42948a1f4ade0e664ef5",
            "6bc95fb33cc6429f9b3c19b45f57915d",
            "c7dd52d4702e4d1ebab4b03c0c5dddf2",
            "dd19e19294ab42ff9bd68b1123a52ee4",
            "6c4ec08543a045b88331feef1c630944",
            "aafeb45e26ee45f5b29e15feebbaa23f",
            "afb8273828da464da58f65460a6b8905",
            "b9558a0fee8743979250ec7767bb5996",
            "ccc09889d2b4436188174c42a53f1dce",
            "3e1b4ed8a8bd44df83d23f50bbf85e35"
          ]
        },
        "id": "TB_bI1SGTVSV",
        "outputId": "b2b8d930-ea31-44ae-e65d-1911b9024763"
      },
      "outputs": [],
      "source": [
        "train_dataloader , eval_dataloader ,test_dataloader  = data_loader(dataset,tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs_wutl2TZZg"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1vP85mTMAmc",
        "outputId": "c50549b1-c4bc-41ca-848b-3decf62ac97f"
      },
      "outputs": [],
      "source": [
        "set_seed(RANDOM_STATE)\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "CAMEL_MIX_model,device = initialize_model(save_dir = PY_CUSTOM_CAMEL_MIX_MODEL_DIR,model_name = CAMEL_MIX_model_huggingface_checkpoint )\n",
        "CAMEL_DA_model , _ = initialize_model(save_dir = PY_CUSTOM_CAMEL_MODEL_DIR,model_name = CAMEL_DA_model_huggingface_checkpoint )\n",
        "MARBERT_model ,_ =initialize_model(save_dir = PY_CUSTOM_MARBERT_MODEL_DIR,model_name = MARBERT_model_huggingface_checkpoint )\n",
        "ALANZI_model ,_=initialize_model(save_dir = PY_CUSTOM_ALANZI_MODEL_DIR,model_name = ALANZI_model_huggingface_checkpoint )\n",
        "#Qarib_model ,_=initialize_model(save_dir = PY_CUSTOM_Qarib_MODEL_DIR,model_name = qarib_model_huggingface_checkpoint )\n",
        "#Giga_model ,_=initialize_model(save_dir = PY_CUSTOM_Giga_bert_MODEL_DIR,model_name = Giga_bert_model_huggingface_checkpoint )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXuRlqRnJ2lT",
        "outputId": "18a972ea-bb89-4930-d69d-32e8d800b3d3"
      },
      "outputs": [],
      "source": [
        "best_model_path = \"best_model.pth\"\n",
        "CAMEL_MIX_model = load_model_from_checkpoint(CAMEL_MIX_model,save_dir = PY_CUSTOM_CAMEL_MIX_MODEL_DIR,checkpoint=best_model_path)\n",
        "CAMEL_DA_model =  load_model_from_checkpoint(CAMEL_DA_model,save_dir = PY_CUSTOM_CAMEL_MODEL_DIR,checkpoint=best_model_path)\n",
        "MARBERT_model = load_model_from_checkpoint(MARBERT_model,save_dir = PY_CUSTOM_MARBERT_MODEL_DIR,checkpoint=best_model_path)\n",
        "ALANZI_model = load_model_from_checkpoint(ALANZI_model,save_dir = PY_CUSTOM_ALANZI3_MODEL_DIR,checkpoint=best_model_path)\n",
        "#Qarib_model = load_model_from_checkpoint(Qarib_model,save_dir = PY_CUSTOM_Qarib_MODEL_DIR,checkpoint=best_model_path)\n",
        "#Giga_model = load_model_from_checkpoint(Giga_model,save_dir = PY_CUSTOM_Giga_bert_MODEL_DIR,checkpoint=best_model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TulrHD-VsLn"
      },
      "source": [
        "#### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkWY-sPBT4jr"
      },
      "outputs": [],
      "source": [
        "ensemble_models = [\n",
        "                   #ARABERT_model,\n",
        "                   CAMEL_MIX_model,\n",
        "                   CAMEL_DA_model,\n",
        "                   MARBERT_model,\n",
        "                   ALANZI_model,\n",
        "                   #Qarib_model\n",
        "\n",
        "                   ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLz7mFtsr-E4",
        "outputId": "818c13b8-d57c-41ca-a305-f3ffcb709bb0"
      },
      "outputs": [],
      "source": [
        "for model in ensemble_models:\n",
        "  print(model.model_name)\n",
        "  df = evaluate_test(model, test_dataloader ,save_dir=PY_ENSEMBLE_MODEL_DIR1,device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404,
          "referenced_widgets": [
            "43b78b3842b6409fb39ed270829b5a0e",
            "1d6aa4130c9646fc96eb767a2df127ec",
            "54d01fe2385c422fac4289e16cec135b",
            "4259b22cc5954642a44a6b13a8975b09",
            "3ffbab80b70d49da98772bf1b4c40934",
            "ee649c98c12447d8b5690fcff5303734",
            "03a704e977c040bdacdee7f8e99956e4",
            "2f5b7701a9d6442ba5dd24b391a4cd9e",
            "99a57ff95bca457ca65d11b7006e1799",
            "04877d1d2e0545429caaf9ec02234ce2",
            "60fc296c8c5b4a90aa688b3dd8fabdec",
            "cf89275ac7e0442ca3c7a26bdf53f914",
            "97cf6a941e9840cb955897b82eb7f3ca",
            "3793f2223aa64d928dadf81a6e0d1e67",
            "fc698f9da87c4cdc81d4aa79fcf5a228",
            "aa61ca36a4764f37affcaceaddbbd7f2",
            "003ca9fbe23c4c9286598709a75db60a",
            "9f42d0dbcc6a4614ac2e8d4bf2ba7538",
            "d01fe91f30144031a8d1a499799491d2",
            "efbd4a9931d94f2eb368c3d548181c31",
            "515342749f8447daafe09355cc3460e5",
            "da870ba09bdd4b07ac81066b85ec2339",
            "ff7e499da3be4168a4ca36ee2fc22395",
            "037dfd1724e24e6e8674a5f3df487cbc",
            "bd0a9e92c6e14107b3125bb8d431044a",
            "f0419702749e4de290be6ca9590e6fc9",
            "1054974ee6f04ded8a3fdbfada13c252",
            "1061d916af024b658b71b0225c8eb5bc",
            "d11916c5989749fab15acd4d92e581e7",
            "71ca90f7ac3540d39b5876c16f20b836",
            "d16532f5308e48e79ae0e72f917a2460",
            "7a22aa213821405c9c918759c958c4db",
            "28711e9ed31c4bebbf7bbd64fe4beb29",
            "4053340bd4ab4b53849da9a7ae4da0ce",
            "63b1660d77454c0680aa12aaf8025355",
            "02abf9f31b5640b6bb34f74620546238",
            "a431e1b8633c41449b82219e45f63219",
            "b82a6447b76f43519853a6cf254b9d9d",
            "b9b6aefd31114093be468447ffa34a36",
            "3138f16e0106447495eba2029db9267d",
            "5c1645242e7d41b7a92780e31489f5a8",
            "8d9fd80eb31246089c4750eec142f247",
            "468c1cc746d6489cb58d3898fde55e8d",
            "5a61251c54754fd4b1617a62a585a7f1",
            "3b215f5a325847c8b9485351fd3ee338",
            "9a0657394d3c485cbdeac5d8a9d166dd",
            "6bc8e20f7c2d43918012c161d96c1b34",
            "8be6106fa316401f963d30dcf529094e",
            "250de30a619c4a85904dc818503c5e89",
            "79ca2140449341f6adb83dd396728439",
            "438dcdfc84b64dfbacad7c34cf81d2fb",
            "60a16786990e40f890ee2576de0ec3ed",
            "9a4c3f2a5e3a40deac9b20a1a849e3d2",
            "29d8b1170310422697d53737eead85c4",
            "2c291554dae043258c764c7a35d31a40"
          ]
        },
        "id": "myhrDQvvsACh",
        "outputId": "0b0d0c20-b4e6-41ea-aadd-7df702f0b5b9"
      },
      "outputs": [],
      "source": [
        "tensor , y_pred,df,contribution_percentages = ensemble_bert_predict(ensemble_models, test_dataloader, device,save_dir=PY_ENSEMBLE_MODEL_DIR1 ,is_accuracy=True)\n",
        "df.head(len(ensemble_models))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "IUa6FR5MVsLo",
        "outputId": "c554a095-ddd4-4af9-e02b-0e624655cba6"
      },
      "outputs": [],
      "source": [
        "print_classification_report(y_pred,save_dir=PY_ENSEMBLE_MODEL_DIR1,title='Classification Report - Ensemble Model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Y4QqHwODVsLp",
        "outputId": "d7d02d27-065b-4783-e948-6fc0088312d5"
      },
      "outputs": [],
      "source": [
        "show_confusion_matrix(y_pred,title = 'Confusion Matrix - Ensemble Model',save_dir = PY_ENSEMBLE_MODEL_DIR1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RMXPLeRuMpd3",
        "b9JLrEECf60N",
        "CqyzSmBXp-Wb",
        "bmHXoCZSITw_",
        "ikHQBnVdfub_",
        "Qyec5dCk1V37",
        "LxmOpxKUdcKJ",
        "86RcmAILabTs",
        "1tXKFO_y9_bS",
        "9hemuWBW7HZo",
        "yZhjja1uMayD",
        "MhvyUQlLjqKD",
        "TvGvBo6xFXpf"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
